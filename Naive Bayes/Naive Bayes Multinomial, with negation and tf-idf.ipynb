{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "# An implementation of NaiveBayes Multinomial classifier. \n",
    "# train and test functions require a list of document, label, pairs\n",
    "\n",
    "class NaiveBayesMultinomial:\n",
    "    \n",
    "    \n",
    "    def train(self, training_set):\n",
    "        self.corpus = training_set\n",
    "        self.all_phrases = dict(training_set) # CHANGE THIS BRUH.\n",
    "        self.label_names = list()\n",
    "        self.doc_label_counts = dict()\n",
    "        self.word_label_counts = dict()\n",
    "        self.vocabulary = dict()\n",
    "        # THE FOLLOWING COMPLETES VOCABULARY AND FREQUENCY COUNTS BY DOC AND BY WORD for EACH LABEL_NAME\n",
    "        for doc, label in training_set:\n",
    "            sentiment = label\n",
    "            if sentiment not in self.doc_label_counts: # if new sentiment name encountered, add to relevant dictionaries\n",
    "                self.doc_label_counts[sentiment] = 0\n",
    "                self.word_label_counts[sentiment] = 0\n",
    "                self.label_names.append(sentiment)\n",
    "                \n",
    "            self.doc_label_counts[sentiment] += 1\n",
    "            words = doc.split()\n",
    "            distinct_words = set()\n",
    "            for word in words:\n",
    "                self.word_label_counts[sentiment] += 1\n",
    "                if word not in self.vocabulary:\n",
    "                    self.vocabulary[word] = dict([(label_name, 0) for label_name in self.label_names])\n",
    "                    self.vocabulary[word]['frequency'] = 0\n",
    "                    self.vocabulary[word]['document frequency'] = 0 # used later for idf\n",
    "                if sentiment not in self.vocabulary[word]:\n",
    "                    self.vocabulary[word][sentiment] = 0\n",
    "                    \n",
    "                self.vocabulary[word][sentiment] += 1\n",
    "                self.vocabulary[word]['frequency'] += 1\n",
    "                if word not in distinct_words:\n",
    "                    distinct_words.add(word)\n",
    "            for word in distinct_words:\n",
    "                self.vocabulary[word]['document frequency'] += 1\n",
    "        \n",
    "        \n",
    "        # compute logpriors\n",
    "        self.log_priors = dict()\n",
    "        for sentiment in self.label_names:\n",
    "            print('Computing prior for ', sentiment)\n",
    "            self.log_priors[sentiment] = np.log(self.doc_label_counts[sentiment]/len(training_set[0]))\n",
    "        \n",
    "        self.log_likelihoods = dict()\n",
    "        for word in self.vocabulary:\n",
    "            self.log_likelihoods[word] = dict([(label_name, 0) for label_name in self.label_names])\n",
    "            for sentiment in self.label_names:\n",
    "                if sentiment not in self.vocabulary[word]:\n",
    "                    self.vocabulary[word][sentiment] = 0    \n",
    "                self.log_likelihoods[word][sentiment] = np.log((self.vocabulary[word][sentiment]+1) / \n",
    "                                                               (self.word_label_counts[sentiment] + len(self.vocabulary)))\n",
    "        \n",
    "    \n",
    "    \n",
    "    def test(self, test_set): # again test_set is assumed to be a tuple of documents and corresponding labels\n",
    "        predictions = list()\n",
    "        for doc, label in test_set:\n",
    "            predictions.append(self.classify(doc))\n",
    "        return predictions\n",
    "    \n",
    "   # def test(self, test_set, use_idf): # again test_set is assumed to be a tuple of documents and corresponding labels\n",
    "    #    predictions = list()\n",
    "     #   if use_idf:\n",
    "      #      for doc, label in test_set:\n",
    "       #         predictions.append(self.classify_tfidf(doc))\n",
    "        #    return predictions\n",
    "        \n",
    "     #   for doc, label in test_set:\n",
    "      #      predictions.append(self.classify(doc))\n",
    "       # return predictions\n",
    "    \n",
    "    def classify_idf(self, document): #predicts class taking into account tf-idf\n",
    "        words = document.split()\n",
    "        probs = dict([label_name, 0] for label_name in self.label_names)     \n",
    "        for sentiment in self.label_names:\n",
    "            probs[sentiment] = self.log_priors[sentiment]\n",
    "            for word in words:\n",
    "                if word not in self.vocabulary:\n",
    "                    continue\n",
    "                probs[sentiment] += self.log_likelihoods[word][sentiment]\n",
    "        val = -np.inf\n",
    "        predicted_class = ''\n",
    "        for sentiment in self.label_names:\n",
    "            if (probs[sentiment] > val):\n",
    "                predicted_class = sentiment\n",
    "                val = probs[sentiment]\n",
    "        return predicted_class\n",
    "    \n",
    "    #def idf(self, word)\n",
    "        \n",
    "            \n",
    "    def classify(self, document): # predict class of a single using multinomial naive bayes\n",
    "        words = document.split()\n",
    "        probs = dict([label_name, 0] for label_name in self.label_names)                \n",
    "        for sentiment in self.label_names:\n",
    "            probs[sentiment] = self.log_priors[sentiment]\n",
    "            for word in words:\n",
    "                if word not in self.vocabulary:\n",
    "                    continue\n",
    "                probs[sentiment] += self.log_likelihoods[word][sentiment]\n",
    "        val = -np.inf\n",
    "        predicted_class = ''\n",
    "        for sentiment in self.label_names:\n",
    "            if (probs[sentiment] > val):\n",
    "                predicted_class = sentiment\n",
    "                val = probs[sentiment]\n",
    "        return predicted_class\n",
    "    \n",
    "    def evaluate_prediction(prediction, actual):\n",
    "        print('Commencing evaluation.')\n",
    "        print('There were ', len(self.label_names), ' different classes.')\n",
    "        print('The entire set had ', len(actual), ' examples.')\n",
    "        print('The overall fine-grained error rate all classes was ', RATE)\n",
    "        print('The overall polarity-error rate was ', RATE)\n",
    "        print('\\n')\n",
    "        print('We now provide analysis on each of the classes.')\n",
    "        for name in self.label_names:\n",
    "            print('Now evaluating class ', name)\n",
    "            print('There were ')\n",
    "    \n",
    "            \n",
    "        \n",
    "        \n",
    "            \n",
    "            \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# An implementation of NaiveBayes Multinomial classifier. \n",
    "# train and test functions require a list of document, label, pairs\n",
    "\n",
    "class NaiveBayesComplementMultinomial:\n",
    "    \n",
    "    def train(self, training_set):\n",
    "        self.all_phrases = dict(training_set) # CHANGE THIS BRUH.\n",
    "        self.label_names = list()\n",
    "        self.doc_label_counts = dict()\n",
    "        self.word_label_counts = dict()\n",
    "        self.vocabulary = dict()\n",
    "        # THE FOLLOWING COMPLETES VOCABULARY AND FREQUENCY COUNTS BY DOC AND BY WORD for EACH LABEL_NAME\n",
    "        for doc, label in training_set:\n",
    "            sentiment = label\n",
    "            if sentiment not in self.doc_label_counts: # if new sentiment name encountered, add to relevant dictionaries\n",
    "                self.doc_label_counts[sentiment] = 0\n",
    "                self.word_label_counts[sentiment] = 0\n",
    "                self.label_names.append(sentiment)\n",
    "                \n",
    "            self.doc_label_counts[sentiment] += 1\n",
    "            \n",
    "            words = doc.split()\n",
    "            for word in words:\n",
    "                self.word_label_counts[sentiment] += 1\n",
    "                if word not in self.vocabulary:\n",
    "                    self.vocabulary[word] = dict([(label_name, 0) for label_name in self.label_names])\n",
    "                    self.vocabulary[word]['frequency'] = 0\n",
    "                if sentiment not in self.vocabulary[word]:\n",
    "                    self.vocabulary[word][sentiment] = 0\n",
    "                    \n",
    "                self.vocabulary[word][sentiment] += 1\n",
    "                self.vocabulary[word]['frequency'] += 1\n",
    "        \n",
    "        \n",
    "        # compute logpriors\n",
    "        self.log_priors = dict()\n",
    "        for sentiment in self.label_names:\n",
    "            print('Computing prior for ', sentiment)\n",
    "            self.log_priors[sentiment] = np.log(self.doc_label_counts[sentiment]/len(training_set[0]))\n",
    "        \n",
    "        self.log_likelihoods = dict()\n",
    "        for word in self.vocabulary:\n",
    "            self.log_likelihoods[word] = dict([(label_name, 0) for label_name in self.label_names])\n",
    "            for sentiment in self.label_names:\n",
    "                if sentiment not in self.vocabulary[word]:\n",
    "                    self.vocabulary[word][sentiment] = 0    \n",
    "                self.log_likelihoods[word][sentiment] = -np.log((self.vocabulary[word]['frequency'] - self.vocabulary[word][sentiment]+1) / \n",
    "                                                               (sum(self.word_label_counts.values())-self.word_label_counts[sentiment] + len(self.vocabulary)))\n",
    "          \n",
    "    def test(self, test_set): # again test_set is assumed to be a tuple of documents and corresponding labels\n",
    "        predictions = list()\n",
    "        for doc, label in test_set:\n",
    "            predictions.append(self.classify(doc))\n",
    "        return predictions\n",
    "        \n",
    "            \n",
    "    def classify(self, document): # predict class of a single using multinomial naive bayes\n",
    "        words = document.split()\n",
    "        probs = dict([label_name, 0] for label_name in self.label_names)                \n",
    "        for sentiment in self.label_names:\n",
    "            probs[sentiment] = self.log_priors[sentiment]\n",
    "            for word in words:\n",
    "                if word not in self.vocabulary:\n",
    "                    continue\n",
    "                probs[sentiment] += self.log_likelihoods[word][sentiment]\n",
    "        \n",
    "        val = -np.inf\n",
    "        predicted_class = ''\n",
    "        for sentiment in self.label_names:\n",
    "            if (probs[sentiment] > val):\n",
    "                predicted_class = sentiment\n",
    "                val = probs[sentiment]\n",
    "        return predicted_class\n",
    "    \n",
    "            \n",
    "        \n",
    "        \n",
    "            \n",
    "            \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is a pre-processing function specified for the Stanford Sentiment Treebank. The data was downloaded January 16th, 2018, and the splitting of the train, dev, test data was performed as specified in the readme.txt (for standardization purposes).\n",
    "\n",
    "The function outputs sets of pairs of lists (documents, corresponding labels) as specified for training, dev and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "\n",
    "def preprocessing_SST(dictionary_of_phrases_filepath, sentences_filepath, phrases_sentiments_filepath, splits_filepath):\n",
    "    dictionaryDF = pd.read_table(dictionary_of_phrases_filepath, sep = '|', names = (\"phrase\", \"phrase id\"))\n",
    "    sentencesDF = pd.read_table(sentences_filepath, sep = '\\t')\n",
    "    sentimentsDF = pd.read_table(phrases_sentiments_filepath, sep = '|')\n",
    "    splitsDF = pd.read_csv(splitsFP)\n",
    "    \n",
    "    phrases = dict()\n",
    "    for row in range(len(dictionaryDF)):\n",
    "        phrase = dictionaryDF['phrase'][row]\n",
    "        phraseId = dictionaryDF['phrase id'][row]\n",
    "        sentiment = valToLabel(sentimentsDF['sentiment values'][phraseId])\n",
    "        words = phrase.split()\n",
    "        phrases[phrase] = {\n",
    "            \"id\" : phraseId,\n",
    "            \"sentiment\" : sentiment\n",
    "        }\n",
    "    train_docs = list()\n",
    "    dev_docs = list()\n",
    "    test_docs = list()\n",
    "    for id in sentencesDF['sentence_index']:\n",
    "        sentence = sentencesDF['sentence'][id - 1]\n",
    "        if (splitsDF['splitset_label'][id - 1] == 1):\n",
    "            train_docs.append(sentence)\n",
    "        elif (splitsDF['splitset_label'][id - 1] == 2):\n",
    "            test_docs.append(sentence)\n",
    "        else: \n",
    "            dev_docs.append(sentence)  \n",
    "    \n",
    "    training = makeInputTuples(train_docs, phrases)\n",
    "    dev = makeInputTuples(dev_docs, phrases)\n",
    "    test = makeInputTuples(test_docs, phrases)\n",
    "    # MAKE SURE TO CLEAN UP THE LISTS \n",
    "    return training, dev, test\n",
    "\n",
    "def normalize(doc): # given document, returns normalized, negation-tracked version\n",
    "    terminators = {';', '.', '?', '!', '\\n', ':', ','}\n",
    "    negations = {'not', 'no', 'neither', 'never', 'n\\'t'}\n",
    "    sentence = doc.split()\n",
    "    normalized_doc = ''\n",
    "    neg_flag = ''\n",
    "    for word in sentence:\n",
    "        #print('Considering word ', word)\n",
    "        word = neg_flag + word\n",
    "        if word in negations:\n",
    "            neg_flag = '__NOT__'\n",
    "        if word[-1] in terminators:\n",
    "            neg_flag = ''\n",
    "            word = word[0:-1]\n",
    "        normalized_doc = normalized_doc + ' ' + word\n",
    "    return normalized_doc\n",
    "\n",
    "def makeInputTuples(docs, phrases_dictionary): # given documents, returns a tuple (docs, labels) where docs is all documents with a label and labels are corresponding labels\n",
    "    doc_label_pairs = []\n",
    "    for doc in docs:\n",
    "        label = docToLabel(doc, phrases_dictionary)\n",
    "        if label == 'Not found':\n",
    "            continue\n",
    "        else:\n",
    "            doc = normalize(doc)\n",
    "            doc_label_pairs.append((doc, label))\n",
    "    return doc_label_pairs\n",
    "\n",
    "\n",
    "def docToLabel(doc, phrases_dictionary): # given doc, either returns 'Not found' or the appropriate label\n",
    "    if doc not in phrases_dictionary:\n",
    "        return 'Not found'\n",
    "    else:\n",
    "        return phrases_dictionary[doc]['sentiment']\n",
    "    \n",
    "def valToLabel(val):\n",
    "    \n",
    "    if (val <= 0.2):\n",
    "        label = 'very negative'\n",
    "    elif (val <= 0.4):\n",
    "        label = 'negative'\n",
    "    elif (val <= 0.6):\n",
    "        label = 'neutral'\n",
    "    elif (val <= 0.8):\n",
    "        label = 'positive'\n",
    "    else:\n",
    "        label = 'very positive'\n",
    "    return label\n",
    "\n",
    "def pairsToPairOfLists(list_of_pairs):\n",
    "    list1 = []\n",
    "    list2 = []\n",
    "    for pair in list_of_pairs:\n",
    "        list1.append(pair[0])\n",
    "        list2.append(pair[1])\n",
    "    return (list1, list2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def polarity_error_nb(predictions, labels):\n",
    "   \n",
    "    total = 0\n",
    "    polarity_matches = 0\n",
    "    for i in range(len(labels)):\n",
    "        if labels[i] == 'neutral':\n",
    "            continue\n",
    "        total += 1\n",
    "        if ((labels[i] == 'positive' or labels[i] == 'very positive') \n",
    "            and (predictions[i] == 'positive' or predictions[i] == 'very positive')):\n",
    "            polarity_matches += 1\n",
    "        if ((labels[i] == 'negative' or labels[i] == 'very negative') \n",
    "            and (predictions[i] == 'negative' or predictions[i] == 'very negative')):\n",
    "            polarity_matches += 1\n",
    "    return 1 - polarity_matches / total    \n",
    "    \n",
    "\n",
    "def fine_grained_error(predictions, labels):\n",
    "    matches = 0\n",
    "    for i in range(len(labels)):\n",
    "        if labels[i] == predictions[i]:\n",
    "            matches += 1\n",
    "    return 1 - matches / len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done preprocessing and splitting.\n"
     ]
    }
   ],
   "source": [
    "dictionaryFP = './stanfordSentimentTreebank/dictionary.txt'\n",
    "sentencesFP = './stanfordSentimentTreebank/datasetSentences.txt'\n",
    "sentimentsFP = './stanfordSentimentTreebank/sentiment_labels.txt'\n",
    "splitsFP = './stanfordSentimentTreebank/datasetSplit.txt'\n",
    "\n",
    "train, dev, test = preprocessing_SST(dictionaryFP, sentencesFP, sentimentsFP, splitsFP)\n",
    "print('Done preprocessing and splitting.')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing prior for  positive\n",
      "Computing prior for  very positive\n",
      "Computing prior for  neutral\n",
      "Computing prior for  negative\n",
      "Computing prior for  very negative\n",
      "['positive', 'very positive', 'neutral', 'negative', 'very negative']\n",
      "{'positive': 7.0103118673072293, 'very positive': 6.4216222678065176, 'neutral': 6.6522176598569231, 'negative': 6.9527286446248686, 'very negative': 6.2441669006637364}\n"
     ]
    }
   ],
   "source": [
    "nbm_model = NaiveBayesMultinomial()\n",
    "nbm_model.train(train)\n",
    "print(nbm_model.label_names)\n",
    "print(nbm_model.log_priors)\n",
    "predictions_test = nbm_model.test(test)\n",
    "predictions_train = nbm_model.test(train)\n",
    "predictions_dev = nbm_model.test(dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.24471126357918815\n",
      "0.6023529411764705\n",
      "0.2727272727272727\n",
      "0.6216475095785441\n",
      "0.06653471376370279\n",
      "0.19046445731181472\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(polarity_error_nb(predictions_test, pairsToPairOfLists(test)[1]))\n",
    "print(fine_grained_error(predictions_test, pairsToPairOfLists(test)[1]))\n",
    "\n",
    "\n",
    "print(polarity_error_nb(predictions_dev, pairsToPairOfLists(dev)[1]))\n",
    "print(fine_grained_error(predictions_dev, pairsToPairOfLists(dev)[1]))\n",
    "\n",
    "\n",
    "print(polarity_error_nb(predictions_train, pairsToPairOfLists(train)[1]))\n",
    "print(fine_grained_error(predictions_train, pairsToPairOfLists(train)[1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(\" The Rock is destined to be the 21st Century 's new `` Conan '' and that he 's going to make a splash even greater than Arnold Schwarzenegger  Jean-Claud Van Damme or Steven Segal \", 'positive'), (\" The gorgeously elaborate continuation of `` The Lord of the Rings '' trilogy is so huge that a column of words can not __NOT__adequately __NOT__describe __NOT__co-writer\\\\/director __NOT__Peter __NOT__Jackson __NOT__'s __NOT__expanded __NOT__vision __NOT__of __NOT__J.R.R Tolkien 's Middle-earth \", 'very positive'), (' Singer\\\\/composer Bryan Adams contributes a slew of songs -- a few potential hits  a few more simply intrusive to the story -- but the whole package certainly captures the intended  er  spirit of the piece ', 'positive'), (\" You 'd think by now America would have had enough of plucky British eccentrics with hearts of gold \", 'neutral'), (' Yet the act is still charming here ', 'positive'), (\" Whether or not __NOT__you __NOT__'re __NOT__enlightened __NOT__by __NOT__any __NOT__of __NOT__Derrida __NOT__'s __NOT__lectures __NOT__on __NOT__`` __NOT__the __NOT__other __NOT__'' __NOT__and __NOT__`` __NOT__the __NOT__self __NOT__ '' Derrida is an undeniably fascinating and playful fellow \", 'very positive'), (' Just the labour involved in creating the layered richness of the imagery in this chiaroscuro of madness and light is astonishing ', 'very positive'), (' Part of the charm of Satin Rouge is that it avoids the obvious with humour and lightness ', 'positive'), (\" a screenplay more ingeniously constructed than `` Memento ''\", 'very positive'), (\" `` Extreme Ops '' exceeds expectations \", 'positive'), (' Good fun  good action  good acting  good dialogue  good pace  good cinematography ', 'very positive'), (' You Should Pay Nine Bucks for This  Because you can hear about suffering Afghan refugees on the news and still be unaffected ', 'neutral'), (' Dramas like this make it human ', 'very positive'), (' A thunderous ride at first  quiet cadences of pure finesse are few and far between  their shortage dilutes the potency of otherwise respectable action ', 'neutral'), (' Still  this flick is fun  and host to some truly excellent sequences ', 'very positive'), (' Australian actor\\\\/director John Polson and award-winning English cinematographer Giles Nuttgens make a terrific effort at disguising the obvious with energy and innovation ', 'positive'), (' You walk out of The Good Girl with mixed emotions -- disapproval of Justine combined with a tinge of understanding for her actions ', 'positive'), (\" Post 9\\\\/11 the philosophical message of `` Personal Freedom First '' might not __NOT__be __NOT__as __NOT__palatable __NOT__as __NOT__intended __NOT__\", 'neutral'), (' If you love reading and\\\\/or poetry  then by all means check it out ', 'positive'), (\" You 'll probably love it \", 'very positive')]\n"
     ]
    }
   ],
   "source": [
    "print(train[0:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing prior for  positive\n",
      "Computing prior for  very positive\n",
      "Computing prior for  neutral\n",
      "Computing prior for  negative\n",
      "Computing prior for  very negative\n",
      "['positive', 'very positive', 'neutral', 'negative', 'very negative']\n",
      "{'positive': 7.0103118673072293, 'very positive': 6.4216222678065176, 'neutral': 6.6522176598569231, 'negative': 6.9527286446248686, 'very negative': 6.2441669006637364}\n"
     ]
    }
   ],
   "source": [
    "nbc_model = NaiveBayesComplementMultinomial()\n",
    "nbc_model.train(train)\n",
    "print(nbc_model.label_names)\n",
    "print(nbc_model.log_priors)\n",
    "predictions_complement_test = nbc_model.test(test)\n",
    "predictions_complement_train = nbc_model.test(train)\n",
    "predictions_complement_dev = nbc_model.test(dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2630074328187536\n",
      "0.5971764705882353\n",
      "0.2896969696969697\n",
      "0.6226053639846743\n",
      "0.033952496954932965\n",
      "0.11962547739312557\n"
     ]
    }
   ],
   "source": [
    "print(polarity_error_nb(predictions_complement_test, pairsToPairOfLists(test)[1]))\n",
    "print(fine_grained_error(predictions_complement_test, pairsToPairOfLists(test)[1]))\n",
    "\n",
    "\n",
    "print(polarity_error_nb(predictions_complement_dev, pairsToPairOfLists(dev)[1]))\n",
    "print(fine_grained_error(predictions_complement_dev, pairsToPairOfLists(dev)[1]))\n",
    "\n",
    "\n",
    "print(polarity_error_nb(predictions_complement_train, pairsToPairOfLists(train)[1]))\n",
    "print(fine_grained_error(predictions_complement_train, pairsToPairOfLists(train)[1]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'negative': -5.7701241819103002,\n",
       " 'neutral': -5.8731864071558668,\n",
       " 'positive': -5.8614942883541898,\n",
       " 'very negative': -6.0296580902745456,\n",
       " 'very positive': -6.5189398549232296}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'negative': -3.9136590932069137,\n",
       " 'neutral': -3.9997012004039822,\n",
       " 'positive': -3.7361607534176984,\n",
       " 'very negative': -4.1562546320059797,\n",
       " 'very positive': -4.0185290185992644}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
