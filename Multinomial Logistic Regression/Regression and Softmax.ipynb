{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.util import ngrams\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "# An implementation of Softmax Regression (aka Multinomial logistic regression) classifier. \n",
    "# train and test functions require a pair of lists (document, label), where label is in the format (0,..., 1, ...0) indicating\n",
    "# to which class the label belongs\n",
    "# initiated with n for n-gram \n",
    "\n",
    "# class computes n-grams\n",
    "\n",
    "class SoftmaxRegression:\n",
    "    \n",
    "    def __init__(self, num_grams=3, num_features=1800, learning_rate = 0.3, LAMBDA = 0.03):\n",
    "        self.num_grams = num_grams\n",
    "        self.num_features = num_features  \n",
    "        self.features_built = False\n",
    "        self.learning_rate = learning_rate\n",
    "        self.LAMBDA = LAMBDA\n",
    "        \n",
    "    def train(self, training_set):\n",
    "        print(\"Building features.\")\n",
    "        self.K = len(training_set[1][0])\n",
    "        self.grams_features = self.build_features(training_set[0]) # extends grams_features as necessary\n",
    "        print(\"Features built.\")\n",
    "        print(\"Now converting documents to input matrix.\")\n",
    "        X = self.build_input(training_set[0])\n",
    "        Y = training_set[1]\n",
    "      \n",
    "        self.theta = self.initialize_parameters()\n",
    "        \n",
    "        self.batch_gradient_descent(X, Y, 2000)\n",
    "    \n",
    "    def batch_gradient_descent(self, X, Y, num_iterations = 2000):\n",
    "        m = X.shape[0]\n",
    "        print(m, \"training examples\")\n",
    "        n = self.num_features \n",
    "        self.training_costs = [[],[]]\n",
    "        print(\"Initiating batch gradient descent with \" , num_iterations, \" iterations.\")\n",
    "        for i in range(num_iterations):\n",
    "            H = self.softmax(np.dot(X, self.theta))\n",
    "            if i % 100 == 0:\n",
    "                cost = self.cost(X, Y, H)\n",
    "               # print(\"Loss after \", i, \" iterations is \", cost)\n",
    "                self.training_costs[0].append(i)\n",
    "                self.training_costs[1].append(cost)\n",
    "            grads = self.grads(X, Y, H)\n",
    "            self.theta = self.theta - self.learning_rate * grads\n",
    "            \n",
    "    def test(self, test_set):\n",
    "        test_X = self.build_input(test_set)\n",
    "        test_Y = self.softmax(np.dot(test_X, self.theta)) \n",
    "        max_indices = np.argmax(test_Y, axis = 1)\n",
    "        return max_indices\n",
    "    \n",
    "    def build_input(self, corpus):\n",
    "        X = []\n",
    "        for document in corpus:\n",
    "            X.append(self.doc_to_gram_vec(document))\n",
    "        return np.array(X)\n",
    "    \n",
    "    def build_features(self, corpus):\n",
    "        all_grams = []\n",
    "        for n in range(1, self.num_grams):\n",
    "            ngrams = []\n",
    "            \n",
    "            for doc in corpus:\n",
    "                ngrams.extend(self.compute_grams(doc, n))\n",
    "                \n",
    "            fdist = FreqDist(ngram for ngram in ngrams)\n",
    "            for phrase in fdist.most_common(self.num_features // self.num_grams):\n",
    "                all_grams.append(phrase[0]) # add common n-gram\n",
    "        grams_dict = dict(zip(all_grams, range(len(all_grams)))) # converts into dictionary with positions\n",
    "        self.features_built = True\n",
    "        return grams_dict\n",
    "    \n",
    "    def doc_to_gram_vec(self, doc): # given document, returns vector representing all features\n",
    "        assert self.features_built\n",
    "        doc_vec = np.zeros(self.num_features) \n",
    "        doc_grams = []\n",
    "        for n in range(1, self.num_grams):\n",
    "            doc_grams.extend(self.compute_grams(doc, n))\n",
    "            \n",
    "        for gram in doc_grams:\n",
    "            if gram in self.grams_features:\n",
    "                doc_vec[self.grams_features[gram]] = 1\n",
    "        \n",
    "        return doc_vec\n",
    "        \n",
    "    def compute_grams(self, doc, num_grams):  # given a document, and selected n num_grams, computes all n_grams\n",
    "        tokens = word_tokenize(doc)\n",
    "        if num_grams == 1:\n",
    "            return tokens\n",
    "        else:\n",
    "            return ngrams(tokens, num_grams)        \n",
    "    \n",
    "    def initialize_parameters(self):\n",
    "        theta = np.random.randn(self.num_features, self.K) * 0.01\n",
    "        return theta\n",
    "    \n",
    "    def cost(self, X, Y, H): \n",
    "        m = X.shape[0]\n",
    "        return -(1/m) *(np.sum(np.multiply(Y, H)) + (self.LAMBDA / 2) * np.sum(np.power(self.theta, 2)))\n",
    "    \n",
    "    def grads(self, X, Y, H): # grads will be a matrix\n",
    "        LAMBDA = self.LAMBDA\n",
    "        grads = -np.dot(X.T, (Y-H))\n",
    "        m = X.shape[0]\n",
    "        grads = (1/m) * (grads + LAMBDA * self.theta)\n",
    "        return grads\n",
    "    \n",
    "    def update_parameters(self, grads, learning_rate):\n",
    "        self.theta = self.theta - (learning_rate * grads)\n",
    "        \n",
    "    def sigmoid(self, x):\n",
    "        z = np.exp(x)\n",
    "        z = z / (z+1)\n",
    "        return z\n",
    "    \n",
    "    def predict(self, x):\n",
    "        probs = softmax(np.dot(self.theta.T, x))\n",
    "        return argmax(probs)\n",
    "    \n",
    "    def softmax(self, Z): # Given matrix Z, returns softmax treating each row as a vector\n",
    "        Z = np.exp(Z)\n",
    "        denoms = np.sum(Z, axis = 1)\n",
    "        denoms = denoms.reshape(Z.shape[0], 1)\n",
    "        return Z / denoms\n",
    "    \n",
    "    print(\"done\")\n",
    "        \n",
    "    \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([14, 77], dtype=int32)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.array([[1,2, 3],[4,5, 6]]) \n",
    "np.sum(np.power(X, 2), axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is a pre-processing step. Used from the NB multinomial code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "\n",
    "def preprocessing_SST(dictionary_of_phrases_filepath, sentences_filepath, phrases_sentiments_filepath, splits_filepath):\n",
    "    dictionaryDF = pd.read_table(dictionary_of_phrases_filepath, sep = '|', names = (\"phrase\", \"phrase id\"))\n",
    "    sentencesDF = pd.read_table(sentences_filepath, sep = '\\t')\n",
    "    sentimentsDF = pd.read_table(phrases_sentiments_filepath, sep = '|')\n",
    "    splitsDF = pd.read_csv(splitsFP)\n",
    "    \n",
    "    phrases = dict()\n",
    "    for row in range(len(dictionaryDF)):\n",
    "        phrase = dictionaryDF['phrase'][row]\n",
    "        phraseId = dictionaryDF['phrase id'][row]\n",
    "        sentiment = valToLabel(sentimentsDF['sentiment values'][phraseId])\n",
    "        words = phrase.split()\n",
    "        phrases[phrase] = {\n",
    "            \"id\" : phraseId,\n",
    "            \"sentiment\" : sentiment\n",
    "        }\n",
    "    train_docs = list()\n",
    "    dev_docs = list()\n",
    "    test_docs = list()\n",
    "    for id in sentencesDF['sentence_index']:\n",
    "        sentence = sentencesDF['sentence'][id - 1]\n",
    "        if (splitsDF['splitset_label'][id - 1] == 1):\n",
    "            train_docs.append(sentence)\n",
    "        elif (splitsDF['splitset_label'][id - 1] == 2):\n",
    "            test_docs.append(sentence)\n",
    "        else: \n",
    "            dev_docs.append(sentence)  \n",
    "            \n",
    "    training = pairsToPairOfLists(makeInputTuples(train_docs, phrases))\n",
    "    #training[1] = labeled_data_finegrained(training[1])\n",
    "    test = pairsToPairOfLists(makeInputTuples(test_docs, phrases))\n",
    "    #test[1] = labeled_data_finegrained(test[1])\n",
    "    dev = pairsToPairOfLists(makeInputTuples(dev_docs, phrases))\n",
    "    #dev[1] = labeled_data_finegrained(dev[1])\n",
    "    \n",
    "    # MAKE SURE TO CLEAN UP THE LISTS \n",
    "    return training, dev, test\n",
    "\n",
    "def normalize(doc): # given document, returns normalized, negation-tracked version\n",
    "    terminators = {';', '.', '?', '!', '\\n', ':', ','}\n",
    "    negations = {'not', 'no', 'neither', 'never', 'n\\'t'}\n",
    "    sentence = doc.split()\n",
    "    normalized_doc = ''\n",
    "    neg_flag = ''\n",
    "    for word in sentence:\n",
    "        #print('Considering word ', word)\n",
    "        word = neg_flag + word\n",
    "        if word in negations:\n",
    "            neg_flag = '__NOT__'\n",
    "        if word[-1] in terminators:\n",
    "            neg_flag = ''\n",
    "            word = word[0:-1]\n",
    "        normalized_doc = normalized_doc + ' ' + word\n",
    "    return normalized_doc\n",
    "\n",
    "def makeInputTuples(docs, phrases_dictionary): # given documents, returns a tuple (docs, labels) where docs is all documents with a label and labels are corresponding labels\n",
    "    doc_label_pairs = []\n",
    "    for doc in docs:\n",
    "        label = docToLabel(doc, phrases_dictionary)\n",
    "        if label == 'Not found':\n",
    "            continue\n",
    "        else:\n",
    "            doc = normalize(doc)\n",
    "            doc_label_pairs.append((doc, label))\n",
    "    return doc_label_pairs\n",
    "\n",
    "\n",
    "def docToLabel(doc, phrases_dictionary): # given doc, either returns 'Not found' or the appropriate label\n",
    "    if doc not in phrases_dictionary:\n",
    "        return 'Not found'\n",
    "    else:\n",
    "        return phrases_dictionary[doc]['sentiment']\n",
    "    \n",
    "def valToLabel(val):\n",
    "    \n",
    "    if (val <= 0.2):\n",
    "        label = 'very negative'\n",
    "    elif (val <= 0.4):\n",
    "        label = 'negative'\n",
    "    elif (val <= 0.6):\n",
    "        label = 'neutral'\n",
    "    elif (val <= 0.8):\n",
    "        label = 'positive'\n",
    "    else:\n",
    "        label = 'very positive'\n",
    "    return label\n",
    "\n",
    "def pairsToPairOfLists(list_of_pairs):\n",
    "    list1 = []\n",
    "    list2 = []\n",
    "    for pair in list_of_pairs:\n",
    "        list1.append(pair[0])\n",
    "        list2.append(pair[1])\n",
    "    return [list1, list2]\n",
    "\n",
    "def labeled_data_finegrained(labels):  #output Y\n",
    "    conversion = {\n",
    "        'very negative' : 0,\n",
    "        'negative' : 1,\n",
    "        'neutral' : 2,\n",
    "        'positive' : 3,\n",
    "        'very positive' : 4\n",
    "    }\n",
    "    Y = list()\n",
    "    for label in labels:\n",
    "        y = [0, 0, 0, 0, 0]\n",
    "        y[conversion[label]] = 1\n",
    "        Y.append(y)\n",
    "    return Y\n",
    "\n",
    "def maxIndicesToLabels(max_indices):\n",
    "    conversion = {\n",
    "        0 : 'very negative',\n",
    "        1 : 'negative',\n",
    "        2 : 'neutral',\n",
    "        3 : 'positive',\n",
    "        4 : 'very positive'\n",
    "    }\n",
    "    labels = []\n",
    "    for index in max_indices:\n",
    "        labels.append(conversion[index])\n",
    "    return labels\n",
    "        \n",
    "print(\"done\")\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def polarity_error_nb(predictions, labels):\n",
    "   \n",
    "    total = 0\n",
    "    polarity_matches = 0\n",
    "    for i in range(len(labels)):\n",
    "        if labels[i] == 'neutral':\n",
    "            continue\n",
    "        total += 1\n",
    "        if ((labels[i] == 'positive' or labels[i] == 'very positive') \n",
    "            and (predictions[i] == 'positive' or predictions[i] == 'very positive')):\n",
    "            polarity_matches += 1\n",
    "        if ((labels[i] == 'negative' or labels[i] == 'very negative') \n",
    "            and (predictions[i] == 'negative' or predictions[i] == 'very negative')):\n",
    "            polarity_matches += 1\n",
    "    return 1 - polarity_matches / total    \n",
    "    \n",
    "\n",
    "def fine_grained_error(predictions, labels):\n",
    "    matches = 0\n",
    "    for i in range(len(labels)):\n",
    "        if labels[i] == predictions[i]:\n",
    "            matches += 1\n",
    "    return 1 - matches / len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done preprocessing and splitting.\n"
     ]
    }
   ],
   "source": [
    "dictionaryFP = './stanfordSentimentTreebank/dictionary.txt'\n",
    "sentencesFP = './stanfordSentimentTreebank/datasetSentences.txt'\n",
    "sentimentsFP = './stanfordSentimentTreebank/sentiment_labels.txt'\n",
    "splitsFP = './stanfordSentimentTreebank/datasetSplit.txt'\n",
    "\n",
    "train, dev, test = preprocessing_SST(dictionaryFP, sentencesFP, sentimentsFP, splitsFP)\n",
    "print('Done preprocessing and splitting.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building features.\n",
      "Features built.\n",
      "Now converting documents to input matrix.\n",
      "8117 training examples\n",
      "Initiating batch gradient descent with  2000  iterations.\n"
     ]
    }
   ],
   "source": [
    "softmax = SoftmaxRegression(3, 1800, 0.3, 0.03)\n",
    "softmax.train([train[0], labeled_data_finegrained(train[1])])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "answers = softmax.test(test[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labels = maxIndicesToLabels(answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_answers = maxIndicesToLabels(softmax.test(train[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.34019439679817043"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "polarity_error_nb(labels, test[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 3, 1, ..., 0, 3, 3], dtype=int64)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1800, 5)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax.theta.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following plots the cost over number of iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plot' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-80-9ffce076ef3b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining_costs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msoftmax\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining_costs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'plot' is not defined"
     ]
    }
   ],
   "source": [
    "plot(softmax.training_costs[0], softmax.training_costs[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now with the model implemented, we do some hyperparameter coarse-fine tuning. We will do so by generating parameter values for the learning_rate, lambda, number of features, and number of grams to use. For each specified set of parameters, we will train the model and compute the result on the dev set. We will select the parameters that perform the best on the dev set.\n",
    "Note that we begin with the defaults num_grams = 3, num_features = 1800, learning_rate = 0.3, lambda = 0.03"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "defaults = [3, 1800, 0.3, 0.09]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data, train_data_labels, train_sentiment_labels = train[0], labeled_data_finegrained(train[1]), train[1]\n",
    "dev_data, dev_data_labels, dev_sentiment_labels = dev[0], labeled_data_finegrained(dev[1]), dev[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Establish initial results\n",
    "init = SoftmaxRegression(defaults[0], defaults[1], defaults[2], defaults[3])\n",
    "init.train([dev_data, dev_data_labels])\n",
    "labels_init = maxIndicesToLabels(init.test(dev_data))\n",
    "init_results = polarity_error_nb(labels_init, dev_sentiment_labels)\n",
    "\n",
    "# Do a coarse search\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
