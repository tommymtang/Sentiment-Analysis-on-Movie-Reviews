{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.util import ngrams\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "# An implementation of Softmax Regression (aka Multinomial logistic regression) classifier. \n",
    "# train and test functions require a pair of lists (document, label), where label is in the format (0,..., 1, ...0) indicating\n",
    "# to which class the label belongs\n",
    "# initiated with n for n-gram \n",
    "\n",
    "# class computes n-grams\n",
    "\n",
    "class SoftmaxRegression:\n",
    "    \n",
    "    def __init__(self, num_grams, num_features):\n",
    "        self.num_grams = num_grams\n",
    "        self.num_features = num_features  \n",
    "        self.features_built = False\n",
    "        \n",
    "    def train(self, training_set):\n",
    "        print(\"Building features.\")\n",
    "        self.K = len(training_set[1][0])\n",
    "        self.grams_features = self.build_features(training_set[0]) # extends grams_features as necessary\n",
    "        print(\"Features built.\")\n",
    "        print(\"Now converting documents to input matrix.\")\n",
    "        X = self.build_input(training_set[0])\n",
    "        Y = training_set[1]\n",
    "      \n",
    "        self.theta = self.initialize_parameters()\n",
    "        \n",
    "        self.batch_gradient_descent(X, Y, 2000)\n",
    "    \n",
    "    def batch_gradient_descent(self, X, Y, num_iterations = 2000, learning_rate = 0.3):\n",
    "        m = X.shape[1]\n",
    "        n = self.num_features  \n",
    "        print(\"Initiating batch gradient descent with \" , num_iterations, \" iterations.\")\n",
    "        for i in range(num_iterations):\n",
    "            if i % 50 == 0:\n",
    "                print(\"completed \" , i , \" iterations\")\n",
    "            H = self.softmax(np.dot(X, self.theta))\n",
    "            grads = self.grads(X, Y, H)\n",
    "            self.theta = self.theta - learning_rate * grads\n",
    "            \n",
    "    def test(self, test_set):\n",
    "        test_X = self.build_input(test_set)\n",
    "        test_Y = self.softmax(np.dot(test_X, self.theta)) \n",
    "        max_indices = np.argmax(test_Y, axis = 1)\n",
    "        return max_indices\n",
    "    \n",
    "    def build_input(self, corpus):\n",
    "        X = []\n",
    "        for document in corpus:\n",
    "            X.append(self.doc_to_gram_vec(document))\n",
    "        return np.array(X)\n",
    "    \n",
    "    def build_features(self, corpus):\n",
    "        all_grams = []\n",
    "        for n in range(1, self.num_grams):\n",
    "            ngrams = []\n",
    "            \n",
    "            for doc in corpus:\n",
    "                ngrams.extend(self.compute_grams(doc, n))\n",
    "                \n",
    "            fdist = FreqDist(ngram for ngram in ngrams)\n",
    "            for phrase in fdist.most_common(self.num_features // self.num_grams):\n",
    "                all_grams.append(phrase[0]) # add common n-gram\n",
    "        grams_dict = dict(zip(all_grams, range(len(all_grams)))) # converts into dictionary with positions\n",
    "        self.features_built = True\n",
    "        return grams_dict\n",
    "    \n",
    "    def doc_to_gram_vec(self, doc): # given document, returns vector representing all features\n",
    "        assert self.features_built\n",
    "        doc_vec = np.zeros(self.num_features) \n",
    "        doc_grams = []\n",
    "        for n in range(1, self.num_grams):\n",
    "            doc_grams.extend(self.compute_grams(doc, n))\n",
    "            \n",
    "        for gram in doc_grams:\n",
    "            if gram in self.grams_features:\n",
    "                doc_vec[self.grams_features[gram]] = 1\n",
    "        \n",
    "        return doc_vec\n",
    "        \n",
    "    def compute_grams(self, doc, num_grams):  # given a document, and selected n num_grams, computes all n_grams\n",
    "        tokens = word_tokenize(doc)\n",
    "        if num_grams == 1:\n",
    "            return tokens\n",
    "        else:\n",
    "            return ngrams(tokens, num_grams)        \n",
    "    \n",
    "    def initialize_parameters(self):\n",
    "        theta = np.random.randn(self.num_features, self.K) * 0.01\n",
    "        return theta\n",
    "    \n",
    "    def linear_forward(self, W, x, b): # forward pass, representing a single layer\n",
    "        assert shape(x)[0] == shape(W)[1]\n",
    "        assert shape(x) == shape(b)\n",
    "        return np.dot(W,x) + b\n",
    "    \n",
    "    def forward_pass(self, X):\n",
    "        np.dot(X, self.theta)\n",
    "        \n",
    "    \n",
    "    def cost(self, x, y, theta, LAMBDA = 0): \n",
    "        J = softmax(np.dot(theta.T, x))[y]\n",
    "    \n",
    "    def grads(self, X, Y, H, LAMBDA = 1.5): # grads will be a matrix\n",
    "        grads = -np.dot(X.T, (Y-H))\n",
    "        m = X.shape[0]\n",
    "        grads = (1/m) * grads      \n",
    "        return grads\n",
    "    \n",
    "    def update_parameters(self, grads, learning_rate):\n",
    "        self.theta = self.theta - (learning_rate * grads)\n",
    "        \n",
    "    def sigmoid(self, x):\n",
    "        z = np.exp(x)\n",
    "        z = z / (z+1)\n",
    "        return z\n",
    "    \n",
    "    def predict(self, x):\n",
    "        probs = softmax(np.dot(self.theta.T, x))\n",
    "        return argmax(probs)\n",
    "    \n",
    "    def softmax(self, Z): # Given matrix Z, returns softmax treating each row as a vector\n",
    "        Z = np.exp(Z)\n",
    "        denoms = np.sum(Z, axis = 1)\n",
    "        denoms = denoms.reshape(Z.shape[0], 1)\n",
    "        return Z / denoms\n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 2], dtype=int64)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = np.zeros(5)\n",
    "s = np.zeros(5)\n",
    "s[2] = 1\n",
    "t[3] = 1\n",
    "w = []\n",
    "w.append(t)\n",
    "w.append(s)\n",
    "w = np.array(w)\n",
    "q = np.sum(w, axis = 1).reshape(w.shape[0], 1)\n",
    "j = w / q\n",
    "maxes = np.argmax(j, axis = 1)\n",
    "maxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is a pre-processing step. Used from the NB multinomial code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "\n",
    "def preprocessing_SST(dictionary_of_phrases_filepath, sentences_filepath, phrases_sentiments_filepath, splits_filepath):\n",
    "    dictionaryDF = pd.read_table(dictionary_of_phrases_filepath, sep = '|', names = (\"phrase\", \"phrase id\"))\n",
    "    sentencesDF = pd.read_table(sentences_filepath, sep = '\\t')\n",
    "    sentimentsDF = pd.read_table(phrases_sentiments_filepath, sep = '|')\n",
    "    splitsDF = pd.read_csv(splitsFP)\n",
    "    \n",
    "    phrases = dict()\n",
    "    for row in range(len(dictionaryDF)):\n",
    "        phrase = dictionaryDF['phrase'][row]\n",
    "        phraseId = dictionaryDF['phrase id'][row]\n",
    "        sentiment = valToLabel(sentimentsDF['sentiment values'][phraseId])\n",
    "        words = phrase.split()\n",
    "        phrases[phrase] = {\n",
    "            \"id\" : phraseId,\n",
    "            \"sentiment\" : sentiment\n",
    "        }\n",
    "    train_docs = list()\n",
    "    dev_docs = list()\n",
    "    test_docs = list()\n",
    "    for id in sentencesDF['sentence_index']:\n",
    "        sentence = sentencesDF['sentence'][id - 1]\n",
    "        if (splitsDF['splitset_label'][id - 1] == 1):\n",
    "            train_docs.append(sentence)\n",
    "        elif (splitsDF['splitset_label'][id - 1] == 2):\n",
    "            test_docs.append(sentence)\n",
    "        else: \n",
    "            dev_docs.append(sentence)  \n",
    "            \n",
    "    training = pairsToPairOfLists(makeInputTuples(train_docs, phrases))\n",
    "    #training[1] = labeled_data_finegrained(training[1])\n",
    "    test = pairsToPairOfLists(makeInputTuples(test_docs, phrases))\n",
    "    #test[1] = labeled_data_finegrained(test[1])\n",
    "    dev = pairsToPairOfLists(makeInputTuples(dev_docs, phrases))\n",
    "    #dev[1] = labeled_data_finegrained(dev[1])\n",
    "    \n",
    "    # MAKE SURE TO CLEAN UP THE LISTS \n",
    "    return training, dev, test\n",
    "\n",
    "def normalize(doc): # given document, returns normalized, negation-tracked version\n",
    "    terminators = {';', '.', '?', '!', '\\n', ':', ','}\n",
    "    negations = {'not', 'no', 'neither', 'never', 'n\\'t'}\n",
    "    sentence = doc.split()\n",
    "    normalized_doc = ''\n",
    "    neg_flag = ''\n",
    "    for word in sentence:\n",
    "        #print('Considering word ', word)\n",
    "        word = neg_flag + word\n",
    "        if word in negations:\n",
    "            neg_flag = '__NOT__'\n",
    "        if word[-1] in terminators:\n",
    "            neg_flag = ''\n",
    "            word = word[0:-1]\n",
    "        normalized_doc = normalized_doc + ' ' + word\n",
    "    return normalized_doc\n",
    "\n",
    "def makeInputTuples(docs, phrases_dictionary): # given documents, returns a tuple (docs, labels) where docs is all documents with a label and labels are corresponding labels\n",
    "    doc_label_pairs = []\n",
    "    for doc in docs:\n",
    "        label = docToLabel(doc, phrases_dictionary)\n",
    "        if label == 'Not found':\n",
    "            continue\n",
    "        else:\n",
    "            doc = normalize(doc)\n",
    "            doc_label_pairs.append((doc, label))\n",
    "    return doc_label_pairs\n",
    "\n",
    "\n",
    "def docToLabel(doc, phrases_dictionary): # given doc, either returns 'Not found' or the appropriate label\n",
    "    if doc not in phrases_dictionary:\n",
    "        return 'Not found'\n",
    "    else:\n",
    "        return phrases_dictionary[doc]['sentiment']\n",
    "    \n",
    "def valToLabel(val):\n",
    "    \n",
    "    if (val <= 0.2):\n",
    "        label = 'very negative'\n",
    "    elif (val <= 0.4):\n",
    "        label = 'negative'\n",
    "    elif (val <= 0.6):\n",
    "        label = 'neutral'\n",
    "    elif (val <= 0.8):\n",
    "        label = 'positive'\n",
    "    else:\n",
    "        label = 'very positive'\n",
    "    return label\n",
    "\n",
    "def pairsToPairOfLists(list_of_pairs):\n",
    "    list1 = []\n",
    "    list2 = []\n",
    "    for pair in list_of_pairs:\n",
    "        list1.append(pair[0])\n",
    "        list2.append(pair[1])\n",
    "    return [list1, list2]\n",
    "\n",
    "def labeled_data_finegrained(labels):  #output Y\n",
    "    conversion = {\n",
    "        'very negative' : 0,\n",
    "        'negative' : 1,\n",
    "        'neutral' : 2,\n",
    "        'positive' : 3,\n",
    "        'very positive' : 4\n",
    "    }\n",
    "    Y = list()\n",
    "    for label in labels:\n",
    "        y = [0, 0, 0, 0, 0]\n",
    "        y[conversion[label]] = 1\n",
    "        Y.append(y)\n",
    "    return Y\n",
    "\n",
    "def maxIndicesToLabels(max_indices):\n",
    "    conversion = {\n",
    "        0 : 'very negative',\n",
    "        1 : 'negative',\n",
    "        2 : 'neutral',\n",
    "        3 : 'positive',\n",
    "        4 : 'very positive'\n",
    "    }\n",
    "    labels = []\n",
    "    for index in max_indices:\n",
    "        labels.append(conversion[index])\n",
    "    return labels\n",
    "        \n",
    "print(\"done\")\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def polarity_error_nb(predictions, labels):\n",
    "   \n",
    "    total = 0\n",
    "    polarity_matches = 0\n",
    "    for i in range(len(labels)):\n",
    "        if labels[i] == 'neutral':\n",
    "            continue\n",
    "        total += 1\n",
    "        if ((labels[i] == 'positive' or labels[i] == 'very positive') \n",
    "            and (predictions[i] == 'positive' or predictions[i] == 'very positive')):\n",
    "            polarity_matches += 1\n",
    "        if ((labels[i] == 'negative' or labels[i] == 'very negative') \n",
    "            and (predictions[i] == 'negative' or predictions[i] == 'very negative')):\n",
    "            polarity_matches += 1\n",
    "    return 1 - polarity_matches / total    \n",
    "    \n",
    "\n",
    "def fine_grained_error(predictions, labels):\n",
    "    matches = 0\n",
    "    for i in range(len(labels)):\n",
    "        if labels[i] == predictions[i]:\n",
    "            matches += 1\n",
    "    return 1 - matches / len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done preprocessing and splitting.\n"
     ]
    }
   ],
   "source": [
    "dictionaryFP = './stanfordSentimentTreebank/dictionary.txt'\n",
    "sentencesFP = './stanfordSentimentTreebank/datasetSentences.txt'\n",
    "sentimentsFP = './stanfordSentimentTreebank/sentiment_labels.txt'\n",
    "splitsFP = './stanfordSentimentTreebank/datasetSplit.txt'\n",
    "\n",
    "train, dev, test = preprocessing_SST(dictionaryFP, sentencesFP, sentimentsFP, splitsFP)\n",
    "print('Done preprocessing and splitting.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building features.\n",
      "Features built.\n",
      "Now converting documents to input matrix.\n",
      "Initiating batch gradient descent with  2000  iterations.\n",
      "completed  0  iterations\n",
      "completed  50  iterations\n",
      "completed  100  iterations\n",
      "completed  150  iterations\n",
      "completed  200  iterations\n",
      "completed  250  iterations\n",
      "completed  300  iterations\n",
      "completed  350  iterations\n",
      "completed  400  iterations\n",
      "completed  450  iterations\n",
      "completed  500  iterations\n",
      "completed  550  iterations\n",
      "completed  600  iterations\n",
      "completed  650  iterations\n",
      "completed  700  iterations\n",
      "completed  750  iterations\n",
      "completed  800  iterations\n",
      "completed  850  iterations\n",
      "completed  900  iterations\n",
      "completed  950  iterations\n",
      "completed  1000  iterations\n",
      "completed  1050  iterations\n",
      "completed  1100  iterations\n",
      "completed  1150  iterations\n",
      "completed  1200  iterations\n",
      "completed  1250  iterations\n",
      "completed  1300  iterations\n",
      "completed  1350  iterations\n",
      "completed  1400  iterations\n",
      "completed  1450  iterations\n",
      "completed  1500  iterations\n",
      "completed  1550  iterations\n",
      "completed  1600  iterations\n",
      "completed  1650  iterations\n",
      "completed  1700  iterations\n",
      "completed  1750  iterations\n",
      "completed  1800  iterations\n",
      "completed  1850  iterations\n",
      "completed  1900  iterations\n",
      "completed  1950  iterations\n"
     ]
    }
   ],
   "source": [
    "softmax = SoftmaxRegression(3, 1800)\n",
    "softmax.train([train[0], labeled_data_finegrained(train[1])])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers = softmax.test(test[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labels = maxIndicesToLabels(answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_answers = maxIndicesToLabels(softmax.test(train[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.34076615208690675"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "polarity_error_nb(labels, test[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 3, 1, ..., 0, 3, 4], dtype=int64)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.21181405,  0.05186096,  0.11044354,  0.20229144, -0.14033383],\n",
       "       [-0.01493069, -0.10321929, -0.35139686,  0.16178347,  0.31833489],\n",
       "       [ 0.08796039, -0.02300391, -0.00071382,  0.00483279, -0.09672029],\n",
       "       ..., \n",
       "       [ 0.01779515,  0.00899588,  0.00743604, -0.0035459 , -0.00808997],\n",
       "       [ 0.00909743, -0.0092661 , -0.00228983,  0.00644285,  0.01114028],\n",
       "       [ 0.01552446,  0.00431155, -0.00384072, -0.01954182,  0.00473385]])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax.theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
