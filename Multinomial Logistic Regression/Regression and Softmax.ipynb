{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.util import ngrams\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "# An implementation of Softmax Regression (aka Multinomial logistic regression) classifier. \n",
    "# train and test functions require a pair of lists (document, label), where label is in the format (0,..., 1, ...0) indicating\n",
    "# to which class the label belongs\n",
    "# initiated with n for n-gram \n",
    "\n",
    "# class computes n-grams\n",
    "\n",
    "class SoftmaxRegression:\n",
    "    \n",
    "    def __init__(self, num_grams=3, num_features=1800, learning_rate = 0.3, LAMBDA = 0.03):\n",
    "        self.num_grams = num_grams\n",
    "        self.num_features = num_features  \n",
    "        self.features_built = False\n",
    "        self.learning_rate = learning_rate\n",
    "        self.LAMBDA = LAMBDA\n",
    "        \n",
    "    def train(self, training_set):\n",
    "        print(\"Building features.\")\n",
    "        self.K = len(training_set[1][0])\n",
    "        self.grams_features = self.build_features(training_set[0]) # extends grams_features as necessary\n",
    "        print(\"Features built.\")\n",
    "        print(\"Now converting documents to input matrix.\")\n",
    "        X = self.build_input(training_set[0])\n",
    "        Y = training_set[1]\n",
    "      \n",
    "        self.theta = self.initialize_parameters()\n",
    "        \n",
    "        self.batch_gradient_descent(X, Y, 2000)\n",
    "    \n",
    "    def batch_gradient_descent(self, X, Y, num_iterations = 2000):\n",
    "        m = X.shape[0]\n",
    "        print(m, \"training examples\")\n",
    "        n = self.num_features \n",
    "        self.training_costs = [[],[]]\n",
    "        print(\"Initiating batch gradient descent with \" , num_iterations, \" iterations.\")\n",
    "        for i in range(num_iterations):\n",
    "            H = self.softmax(np.dot(X, self.theta))\n",
    "            if i % 100 == 0:\n",
    "                cost = self.cost(X, Y, H)\n",
    "               # print(\"Loss after \", i, \" iterations is \", cost)\n",
    "                self.training_costs[0].append(i)\n",
    "                self.training_costs[1].append(cost)\n",
    "            grads = self.grads(X, Y, H)\n",
    "            self.theta = self.theta - self.learning_rate * grads\n",
    "            \n",
    "    def test(self, test_set):\n",
    "        test_X = self.build_input(test_set)\n",
    "        test_Y = self.softmax(np.dot(test_X, self.theta)) \n",
    "        max_indices = np.argmax(test_Y, axis = 1)\n",
    "        return max_indices\n",
    "    \n",
    "    def build_input(self, corpus):\n",
    "        X = []\n",
    "        for document in corpus:\n",
    "            X.append(self.doc_to_gram_vec(document))\n",
    "        return np.array(X)\n",
    "    \n",
    "    def build_features(self, corpus):\n",
    "        all_grams = []\n",
    "        for n in range(1, self.num_grams):\n",
    "            ngrams = []\n",
    "            \n",
    "            for doc in corpus:\n",
    "                ngrams.extend(self.compute_grams(doc, n))\n",
    "                \n",
    "            fdist = FreqDist(ngram for ngram in ngrams)\n",
    "            for phrase in fdist.most_common(self.num_features // self.num_grams):\n",
    "                all_grams.append(phrase[0]) # add common n-gram\n",
    "        grams_dict = dict(zip(all_grams, range(len(all_grams)))) # converts into dictionary with positions\n",
    "        self.features_built = True\n",
    "        return grams_dict\n",
    "    \n",
    "    def doc_to_gram_vec(self, doc): # given document, returns vector representing all features\n",
    "        assert self.features_built\n",
    "        doc_vec = np.zeros(self.num_features) \n",
    "        doc_grams = []\n",
    "        for n in range(1, self.num_grams):\n",
    "            doc_grams.extend(self.compute_grams(doc, n))\n",
    "            \n",
    "        for gram in doc_grams:\n",
    "            if gram in self.grams_features:\n",
    "                doc_vec[self.grams_features[gram]] = 1\n",
    "        \n",
    "        return doc_vec\n",
    "        \n",
    "    def compute_grams(self, doc, num_grams):  # given a document, and selected n num_grams, computes all n_grams\n",
    "        tokens = word_tokenize(doc)\n",
    "        if num_grams == 1:\n",
    "            return tokens\n",
    "        else:\n",
    "            return ngrams(tokens, num_grams)        \n",
    "    \n",
    "    def initialize_parameters(self):\n",
    "        theta = np.random.randn(self.num_features, self.K) * 0.01\n",
    "        return theta\n",
    "    \n",
    "    def cost(self, X, Y, H): \n",
    "        m = X.shape[0]\n",
    "        return -(1/m) *(np.sum(np.multiply(Y, H)) + (self.LAMBDA / 2) * np.sum(np.power(self.theta, 2)))\n",
    "    \n",
    "    def grads(self, X, Y, H): # grads will be a matrix\n",
    "        LAMBDA = self.LAMBDA\n",
    "        grads = -np.dot(X.T, (Y-H))\n",
    "        m = X.shape[0]\n",
    "        grads = (1/m) * (grads + LAMBDA * np.absolute(self.theta))\n",
    "        return grads\n",
    "    \n",
    "    def update_parameters(self, grads, learning_rate):\n",
    "        self.theta = self.theta - (learning_rate * grads)\n",
    "        \n",
    "    def sigmoid(self, x):\n",
    "        z = np.exp(x)\n",
    "        z = z / (z+1)\n",
    "        return z\n",
    "    \n",
    "    def predict(self, x):\n",
    "        probs = softmax(np.dot(self.theta.T, x))\n",
    "        return argmax(probs)\n",
    "    \n",
    "    def softmax(self, Z): # Given matrix Z, returns softmax treating each row as a vector\n",
    "        Z = np.exp(Z)\n",
    "        denoms = np.sum(Z, axis = 1)\n",
    "        denoms = denoms.reshape(Z.shape[0], 1)\n",
    "        return Z / denoms\n",
    "    \n",
    "    print(\"done\")\n",
    "        \n",
    "    \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([14, 77], dtype=int32)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.array([[1,2, 3],[4,5, 6]]) \n",
    "np.sum(np.power(X, 2), axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is a pre-processing step. Used from the NB multinomial code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "\n",
    "def preprocessing_SST(dictionary_of_phrases_filepath, sentences_filepath, phrases_sentiments_filepath, splits_filepath):\n",
    "    dictionaryDF = pd.read_table(dictionary_of_phrases_filepath, sep = '|', names = (\"phrase\", \"phrase id\"))\n",
    "    sentencesDF = pd.read_table(sentences_filepath, sep = '\\t')\n",
    "    sentimentsDF = pd.read_table(phrases_sentiments_filepath, sep = '|')\n",
    "    splitsDF = pd.read_csv(splitsFP)\n",
    "    \n",
    "    phrases = dict()\n",
    "    for row in range(len(dictionaryDF)):\n",
    "        phrase = dictionaryDF['phrase'][row]\n",
    "        phraseId = dictionaryDF['phrase id'][row]\n",
    "        sentiment = valToLabel(sentimentsDF['sentiment values'][phraseId])\n",
    "        words = phrase.split()\n",
    "        phrases[phrase] = {\n",
    "            \"id\" : phraseId,\n",
    "            \"sentiment\" : sentiment\n",
    "        }\n",
    "    train_docs = list()\n",
    "    dev_docs = list()\n",
    "    test_docs = list()\n",
    "    for id in sentencesDF['sentence_index']:\n",
    "        sentence = sentencesDF['sentence'][id - 1]\n",
    "        if (splitsDF['splitset_label'][id - 1] == 1):\n",
    "            train_docs.append(sentence)\n",
    "        elif (splitsDF['splitset_label'][id - 1] == 2):\n",
    "            test_docs.append(sentence)\n",
    "        else: \n",
    "            dev_docs.append(sentence)  \n",
    "            \n",
    "    training = pairsToPairOfLists(makeInputTuples(train_docs, phrases))\n",
    "    #training[1] = labeled_data_finegrained(training[1])\n",
    "    test = pairsToPairOfLists(makeInputTuples(test_docs, phrases))\n",
    "    #test[1] = labeled_data_finegrained(test[1])\n",
    "    dev = pairsToPairOfLists(makeInputTuples(dev_docs, phrases))\n",
    "    #dev[1] = labeled_data_finegrained(dev[1])\n",
    "    \n",
    "    # MAKE SURE TO CLEAN UP THE LISTS \n",
    "    return training, dev, test\n",
    "\n",
    "def normalize(doc): # given document, returns normalized, negation-tracked version\n",
    "    terminators = {';', '.', '?', '!', '\\n', ':', ','}\n",
    "    negations = {'not', 'no', 'neither', 'never', 'n\\'t'}\n",
    "    sentence = doc.split()\n",
    "    normalized_doc = ''\n",
    "    neg_flag = ''\n",
    "    for word in sentence:\n",
    "        #print('Considering word ', word)\n",
    "        word = neg_flag + word\n",
    "        if word in negations:\n",
    "            neg_flag = '__NOT__'\n",
    "        if word[-1] in terminators:\n",
    "            neg_flag = ''\n",
    "            word = word[0:-1]\n",
    "        normalized_doc = normalized_doc + ' ' + word\n",
    "    return normalized_doc\n",
    "\n",
    "def makeInputTuples(docs, phrases_dictionary): # given documents, returns a tuple (docs, labels) where docs is all documents with a label and labels are corresponding labels\n",
    "    doc_label_pairs = []\n",
    "    for doc in docs:\n",
    "        label = docToLabel(doc, phrases_dictionary)\n",
    "        if label == 'Not found':\n",
    "            continue\n",
    "        else:\n",
    "            doc = normalize(doc)\n",
    "            doc_label_pairs.append((doc, label))\n",
    "    return doc_label_pairs\n",
    "\n",
    "\n",
    "def docToLabel(doc, phrases_dictionary): # given doc, either returns 'Not found' or the appropriate label\n",
    "    if doc not in phrases_dictionary:\n",
    "        return 'Not found'\n",
    "    else:\n",
    "        return phrases_dictionary[doc]['sentiment']\n",
    "    \n",
    "def valToLabel(val):\n",
    "    \n",
    "    if (val <= 0.2):\n",
    "        label = 'very negative'\n",
    "    elif (val <= 0.4):\n",
    "        label = 'negative'\n",
    "    elif (val <= 0.6):\n",
    "        label = 'neutral'\n",
    "    elif (val <= 0.8):\n",
    "        label = 'positive'\n",
    "    else:\n",
    "        label = 'very positive'\n",
    "    return label\n",
    "\n",
    "def pairsToPairOfLists(list_of_pairs):\n",
    "    list1 = []\n",
    "    list2 = []\n",
    "    for pair in list_of_pairs:\n",
    "        list1.append(pair[0])\n",
    "        list2.append(pair[1])\n",
    "    return [list1, list2]\n",
    "\n",
    "def labeled_data_finegrained(labels):  #output Y\n",
    "    conversion = {\n",
    "        'very negative' : 0,\n",
    "        'negative' : 1,\n",
    "        'neutral' : 2,\n",
    "        'positive' : 3,\n",
    "        'very positive' : 4\n",
    "    }\n",
    "    Y = list()\n",
    "    for label in labels:\n",
    "        y = [0, 0, 0, 0, 0]\n",
    "        y[conversion[label]] = 1\n",
    "        Y.append(y)\n",
    "    return Y\n",
    "\n",
    "def maxIndicesToLabels(max_indices):\n",
    "    conversion = {\n",
    "        0 : 'very negative',\n",
    "        1 : 'negative',\n",
    "        2 : 'neutral',\n",
    "        3 : 'positive',\n",
    "        4 : 'very positive'\n",
    "    }\n",
    "    labels = []\n",
    "    for index in max_indices:\n",
    "        labels.append(conversion[index])\n",
    "    return labels\n",
    "        \n",
    "print(\"done\")\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def polarity_error_nb(predictions, labels):\n",
    "   \n",
    "    total = 0\n",
    "    polarity_matches = 0\n",
    "    for i in range(len(labels)):\n",
    "        if labels[i] == 'neutral':\n",
    "            continue\n",
    "        total += 1\n",
    "        if ((labels[i] == 'positive' or labels[i] == 'very positive') \n",
    "            and (predictions[i] == 'positive' or predictions[i] == 'very positive')):\n",
    "            polarity_matches += 1\n",
    "        if ((labels[i] == 'negative' or labels[i] == 'very negative') \n",
    "            and (predictions[i] == 'negative' or predictions[i] == 'very negative')):\n",
    "            polarity_matches += 1\n",
    "    return 1 - polarity_matches / total    \n",
    "    \n",
    "\n",
    "def fine_grained_error(predictions, labels):\n",
    "    matches = 0\n",
    "    for i in range(len(labels)):\n",
    "        if labels[i] == predictions[i]:\n",
    "            matches += 1\n",
    "    return 1 - matches / len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done preprocessing and splitting.\n"
     ]
    }
   ],
   "source": [
    "dictionaryFP = './stanfordSentimentTreebank/dictionary.txt'\n",
    "sentencesFP = './stanfordSentimentTreebank/datasetSentences.txt'\n",
    "sentimentsFP = './stanfordSentimentTreebank/sentiment_labels.txt'\n",
    "splitsFP = './stanfordSentimentTreebank/datasetSplit.txt'\n",
    "\n",
    "train, dev, test = preprocessing_SST(dictionaryFP, sentencesFP, sentimentsFP, splitsFP)\n",
    "print('Done preprocessing and splitting.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building features.\n",
      "Features built.\n",
      "Now converting documents to input matrix.\n",
      "8117 training examples\n",
      "Initiating batch gradient descent with  2000  iterations.\n"
     ]
    }
   ],
   "source": [
    "softmax = SoftmaxRegression(3, 1800, 0.3, 0.03)\n",
    "softmax.train([train[0], labeled_data_finegrained(train[1])])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "answers = softmax.test(test[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labels = maxIndicesToLabels(answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3436249285305889"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "polarity_error_nb(labels, test[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 3, 3, ..., 0, 3, 4], dtype=int64)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1800, 5)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax.theta.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following plots the cost over number of iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYYAAAD8CAYAAABzTgP2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAAIABJREFUeJzt3Xl0VfW99/H3NxOEISQhA4EQwhQQ\nBBEiIiJYEbVqxWrVqq1Yi0Nb7Xxb72qfPnd19d5r623to7W11tri1KpUhUodgCo4MBiQUSFhHhIg\nA/Oc5Pv8cTbcJA1J4JCcJOfzWuuss8/ev3P2NztwPtn7t/dvm7sjIiJyQkykCxARkdZFwSAiIrUo\nGEREpBYFg4iI1KJgEBGRWhQMIiJSi4JBRERqUTCIiEgtCgYREaklLtIFnIm0tDTPzc2NdBkiIm3K\nkiVLytw9vbF2bTIYcnNzKSgoiHQZIiJtipltbko7HUoSEZFaFAwiIlKLgkFERGpRMIiISC0KBhER\nqSWsYDCzVDObbWZFwXNKPW1GmNkCM1ttZivM7JYay/qa2aLg/S+aWUI49YiISPjC3WN4EJjr7gOB\nucHrug4Bd7j7UOAq4Ndmlhws+znwSPD+3cBXw6xHRETCFG4wTAamBdPTgOvrNnD3QncvCqaLgV1A\nupkZcBkwvaH3n00zlxfz/KImncYrIhK1wg2GTHcvAQieMxpqbGajgQRgPdAd2OPulcHibUCvMOtp\n0FurdvDY3HXoPtciIqfWaDCY2RwzW1XPY/LprMjMsoBnga+4ezVg9TQ75Te2md1jZgVmVlBaWno6\nqz5pfF4aO/YdoWjXgTN6v4hINGh0SAx3v/xUy8xsp5lluXtJ8MW/6xTtkoBZwI/dfWEwuwxINrO4\nYK8hGyhuoI4ngScB8vPzz+hP/vF5oSFC5q0tJS+z65l8hIhIuxfuoaSZwJRgegowo26D4EyjV4Fn\n3P3lE/M9dDznHeALDb3/bMrqlkheZhfmF53ZHoeISDQINxgeAiaZWREwKXiNmeWb2VNBm5uB8cCd\nZrYseIwIlv0Q+K6ZrSPU5/DHMOtp1PiB6SzaWMHhY1XNvSoRkTYprNFV3b0cmFjP/AJgajD9HPDc\nKd6/ARgdTg2na8KgdJ56fyMLN5bzmUEN9pWLiESlqLvy+YLcVDrGxzBvrQ4niYjUJ+qCoWN8LGP6\ndVc/g4jIKURdMECon2FD6UG2VhyKdCkiIq1OVAbDhEGh01a11yAi8q+iMhj6pXWmV3Ii8wsVDCIi\ndUVlMJgZ4/PS+WBdOcerqiNdjohIqxKVwQAwIS+dA0cr+XjLnkiXIiLSqkRtMIwd0J3YGGNeYb2j\neIiIRK2oDYakjvGMyklhfmFZpEsREWlVojYYIDTa6srteyk7cDTSpYiItBpRHgyh01bfL9Jeg4jI\nCVEdDOf27EZq5wSdtioiUkNUB0NMjHHJwDTmF5VSXa27uomIQJQHA4ROWy07cIxPSvZFuhQRkVYh\n6oPhkoHBXd10OElEBFAwkN61A0N7JqmfQUQkEPXBAKGzk5Zs3s3+I8cjXYqISMQpGAgNw11Z7SxY\nXx7pUkREIk7BAIzqk0LnhFgNwy0igoIBgIS4GC7qn8a8wlLcddqqiEQ3BUNgwqB0tlYcZlO57uom\nItFNwRCYcOK01bUabVVEopuCIZDTvRN90zozX+MmiUiUUzDUMH5gGgvWl3O0sirSpYiIRExYwWBm\nqWY228yKgueUetqMMLMFZrbazFaY2S01lj1vZmvNbJWZPW1m8eHUE67xeekcPl5FwabdkSxDRCSi\nwt1jeBCY6+4DgbnB67oOAXe4+1DgKuDXZpYcLHseGAwMAxKBqWHWE5Yx/bqTEBujq6BFJKqFGwyT\ngWnB9DTg+roN3L3Q3YuC6WJgF5AevP6HB4DFQHaY9YSlc4c48nNTNG6SiES1cIMh091LAILnjIYa\nm9loIAFYX2d+PPBl4M0G3nuPmRWYWUFpafN9cU/IS2fNjv3s3Hek2dYhItKaNRoMZjYn6AOo+5h8\nOisysyzgWeAr7l5dZ/Fvgfnu/t6p3u/uT7p7vrvnp6enn86qT8uJu7ppr0FEolVcYw3c/fJTLTOz\nnWaW5e4lwRd/vRcBmFkSMAv4sbsvrLPs/xI6tHTvaVXeTAb36EpG1w7MLyzl5vzekS5HRKTFhXso\naSYwJZieAsyo28DMEoBXgWfc/eU6y6YCVwK31rMXERFmxvi8dN4rKqNKd3UTkSgUbjA8BEwysyJg\nUvAaM8s3s6eCNjcD44E7zWxZ8BgRLHsCyAQWBPN/EmY9Z8X4vHT2Hj7Oim17Il2KiEiLa/RQUkPc\nvRyYWM/8AoJTT939OeC5U7w/rPU3l0sGpGEW6mc4P+dfLs0QEWnXdOVzPVI6JzA8O1nXM4hIVFIw\nnMKEvHSWbd3D3kO6q5uIRBcFwylMyEuj2uH9dRpUT0Sii4LhFM7LTqZrxzgdThKRqKNgOIW42Bgu\nGai7uolI9FEwNGD8wHR27DtC0a4DkS5FRKTFKBgacHJ4jLU6nCQi0UPB0ICeyYkMzOjC/CIFg4hE\nDwVDIybkpbNoYwWHj+mubiISHRQMjRifl86xymoWbiyPdCkiIi1CwdCI0X1T6RCnu7qJSPRQMDSi\nY3wsY/p11/0ZRCRqKBiaYHxeOhtKD7K14lCkSxERaXYKhiaYEJy2qrOTRCQaKBiaoH96Z3olJ6qf\nQUSigoKhCU7c1e3DdeUcr2oVN5oTEWk2CoYmmpCXxv6jlXy8RXd1E5H2TcHQRGMHpBEbYzqcJCLt\nnoKhiZI6xjMyJ5kZy7ez74hu3iMi7ZeC4TR8/4pBlOw5wndfXEZ1tYbiFpH2ScFwGi7s150fX3MO\ncz7dxaP/LIp0OSIizULBcJqmjM3lxpHZ/HpOEbM/2RnpckREzjoFw2kyM/7z8+cyPLsb33lxGet0\nEx8RaWfCCgYzSzWz2WZWFDyn1NNmhJktMLPVZrbCzG6pp81jZtZmvmE7xsfyxJdG0SEuhnueLWC/\nOqNFpB0Jd4/hQWCuuw8E5gav6zoE3OHuQ4GrgF+bWfKJhWaWDyTX875WrWdyIo/fPpIt5Yf4zovL\n1RktIu1GuMEwGZgWTE8Drq/bwN0L3b0omC4GdgHpAGYWCzwM/CDMOiJizMnO6J3qjBaRdiPcYMh0\n9xKA4DmjocZmNhpIANYHs+4HZp74jEbee4+ZFZhZQWlp67nITJ3RItLeNBoMZjbHzFbV85h8Oisy\nsyzgWeAr7l5tZj2Bm4DHmvJ+d3/S3fPdPT89Pf10Vt2s1BktIu1No8Hg7pe7+7n1PGYAO4Mv/BNf\n/Lvq+wwzSwJmAT9294XB7POBAcA6M9sEdDKzdWfhZ2pxNTuj71VntIi0ceEeSpoJTAmmpwAz6jYw\nswTgVeAZd3/5xHx3n+XuPdw9191zgUPuPiDMeiLmRGf0ZnVGi0gbF24wPARMMrMiYFLwGjPLN7On\ngjY3A+OBO81sWfAYEeZ6W6WandGP/bNN7vyIiBAXzpvdvRyYWM/8AmBqMP0c8FwTPqtLOLW0FlPG\n5rJi+14emVPI0J5JXD4kM9IliYicFl35fJaZGf/1+WEM6xXqjF5fqs5oEWlbFAzNoGN8LL//8igS\n4mK45xl1RotI26JgaCYnOqM3lR/iuy+pM1pE2g4FQzM60Rk9+xN1RotI26FgaGZ3js3lhpG9eGRO\nIXN0ZbSItAEKhmamzmgRaWsUDC2gZmf0V/70EVsrDkW6JBGRU1IwtJCeyYk8NSWfPYeOcfPvF2hM\nJRFptRQMLej8nBRevPcijlc5N/9+Aau27410SSIi/0LB0MLOyUri5fsuIjE+llufXMjijRWRLklE\npBYFQwT0TevMy/ddRHpSB+54ehHvrq13UFoRkYhQMERIz+REXr73Ivqnd+HuZwqYtaLRexWJiLQI\nBUMEde/SgRfuHsN52ck88JelvPjRlkiXJCKiYIi0bonxPPvVCxk3MJ0f/m0lT723IdIliUiUUzC0\nAokJsTx1Rz5XD+vBz2Z9yq9mF+KusZVEJDLCuh+DnD0JcTE8dutIunRYwaNzi9h3+Dg/uXYIMTEW\n6dJEJMooGFqR2BjjoRuG06VDPE9/sJH9Ryr5+Y3DiIvVjp2ItBwFQysTE2P8n2vPoVtiPI/MKeTA\n0eM8euv5dIiLjXRpIhIl9KdoK2RmfOvygfzk2iG8tXonU6cVcOhYZaTLEpEooWBoxe4a15eHvzCc\nD9aV8eU/LmbvYd0JTkSan4KhlbspvzeP3zaSFdv28MUnF1K6/2ikSxKRdk7B0AZ8dlgWT025gI1l\nB7jhdx+wuliD74lI81EwtBET8tL5y91jOF7p3PDbD3npo62RLklE2qmwgsHMUs1stpkVBc8p9bQZ\nYWYLzGy1ma0ws1tqLDMz+08zKzSzT83sm+HU096dn5PC698cR35uCj/42wp+MH05R45XRbosEWln\nwt1jeBCY6+4DgbnB67oOAXe4+1DgKuDXZpYcLLsT6A0MdvdzgL+GWU+7l9alA8/cdSH3f2YALxVs\n44bffsiWct0RTkTOnnCDYTIwLZieBlxft4G7F7p7UTBdDOwC0oPFXwN+6u7VwXKNP90EsTHG968c\nxNN35rN9z2Gueew9Zn+yM9JliUg7EW4wZLp7CUDwnNFQYzMbDSQA64NZ/YFbzKzAzN4ws4ENvPee\noF1BaWlpmGW3D5cNzuT1B8bRp3sn7n6mgJ+/uYbKqupIlyUibVyjwWBmc8xsVT2PyaezIjPLAp4F\nvnJiDwHoABxx93zgD8DTp3q/uz/p7vnunp+enn6qZlGnd2onpt83lltH5/C7d9fz5T8u1imtIhKW\nRoPB3S9393PrecwAdgZf+Ce++Os9FGRmScAs4MfuvrDGom3A34LpV4Hh4fww0apjfCz/fcMw/uem\n8/h4626uefQ9PtqkW4aKyJkJ91DSTGBKMD0FmFG3gZklEPrSf8bdX66z+DXgsmB6AlAYZj1R7Quj\nsnn16xfTKSGWLz65kKfe26Dhu0XktIUbDA8Bk8ysCJgUvMbM8s3sqaDNzcB44E4zWxY8RtR4/41m\nthL4b2BqmPVEvXOykpj5wDguPyeDn836lK8/v5T9RzSUhog0nbXFvyjz8/O9oKAg0mW0au7OU+9t\n5KE315CT2onffWkkg3skRbosEYkgM1sS9Ok2SFc+t1Nmxt3j+/HC1As5cLSS6x//gFeWbot0WSLS\nBigY2rkL+3Vn1jfHMTw7me++tJx/f2UFB45qCG8ROTUFQxTI6NqRF6ZeyH0T+vPXj7Zy5SPzeb+o\nLNJliUgrpWCIEnGxMTz42cFMv+8iOsTF8KU/LuLfX1mpjmkR+RcKhigzqk8q//jWJdw7vh8vfrSF\nKx+Zz7xCXUkuIv9LwRCFOsbH8u9Xn8PfvjaWTh3imPL0Yn4wfbnuECcigIIhqp2fk8LrD4zja5f2\nZ/qSbVz5yHz+uUaD8YlEOwVDlOsYH8sPrxrMa9+4mG6J8dz15wK+99Jy9h7S3oNItFIwCADDs5OZ\n+cDFPHDZAF5btp1Jj8zTUN4iUUrBICd1iIvle1cMYsY3Lia1cwJ3P1PAt//6MbsPHot0aSLSghQM\n8i/O7dWNmfeP49uXD+T1FSVMemQ+b64qiXRZItJCFAxSr4S4GL59eR4z7x9HZlIH7ntuKfe/sJTy\nA7rXg0h7p2CQBg3pmcRr37iY703K463VO5j4q3k8t3AzVdVtb/BFEWkaBYM0Kj42hgcmDmTWNy9h\ncI+u/Pi1VVz72Pss2lAe6dJEpBkoGKTJ8jK78pe7x/D4bSPZd/g4tzy5kPtfWErxnsORLk1EziIF\ng5wWM+Oa4VnM+e4EvjVxILM/2cllv3yXR+cWceR4VaTLE5GzQMEgZyQxIZbvTMpj7vcmcNngDH41\nu5DLfzWPN1eV6HaiIm2cgkHCkp3Sid/ePooX7r6Qzglx3PfcUm5/ahFrd+yPdGkicoYUDHJWjO2f\nxqxvjuOnk4eyungfVz/6Hv8xc7WG1hBpgxQMctbExcZwx0W5vPv9S7l1dG+eWbCJS//nHZ5fpNNb\nRdoSBYOcdSmdE/jZ9cN4/YFLGJjZlR+9uorPPfY+H22qiHRpItIECgZpNkN6JvHiPWP4zW3ns+fQ\nMW56YgH3v7CUjWUHI12aiDQgLtIFSPtmZlw7vCcTB2fyu3nr+cP8Dbyxagc352fzzYkDyeqWGOkS\nRaSOsPYYzCzVzGabWVHwnFJPmxFmtsDMVpvZCjO7pcayiWa21MyWmdn7ZjYgnHqk9UpMiOW7k/KY\n94NL+fKYPkxfso0JD7/Lz17/ROMvibQyFs4552b2C6DC3R8ysweBFHf/YZ02eYC7e5GZ9QSWAOe4\n+x4zKwQmu/unZvZ1YLS739nYevPz872goOCM65bI21pxiEfnFvG3pdtIjI/lq5f0Y+olfUnqGB/p\n0kTaLTNb4u75jbULt49hMjAtmJ4GXF+3gbsXuntRMF0M7ALSTywGkoLpbkBxmPVIG9E7tRMP33Qe\nb39nAhMGpfPo3CLG/+Idfj9vPYeP6QpqkUgKd49hj7sn13i9293/5XBSjeWjCQXIUHevNrNLgNeA\nw8A+YIy772tsvdpjaH9WbtvL/7y9lnmFpWR07cADEwdyS35vEuJ0foTI2dLUPYZGg8HM5gA96ln0\nI2BaU4PBzLKAd4Ep7r4wmPcK8HN3X2Rm/wYMcvepp3j/PcA9ADk5OaM2b97c2M8mbdCiDeU8/NZa\nCjbvJie1E9+ZNJDrzutFbIxFujSRNu+sBUMjK1kLXOruJSe++N19UD3tkgiFwn+7+8vBvHRgobv3\nD17nAG+6+5DG1qs9hvbN3Xm3sJSH31zLJyX7yMvswveuGMQVQzIxU0CInKmW6mOYCUwJpqcAM+op\nJAF4FXjmRCgEdgPdgs5pgEnAp2HWI+2AmfGZQRm8/sA4fnPb+VRWOfc+u4TrH/+A+YWlGqRPpJmF\nu8fQHXgJyAG2ADe5e4WZ5QP3uftUM/sS8CdgdY233unuy8zs88BPgWpCQXGXu29obL3aY4gulVXV\nvLJ0O7+eU0jx3iOcl92Nr39mAJPOySRGh5hEmqxFDiVFioIhOh2trOJvS7bzxLz1bKk4xMCMLnz9\nM/353PCexMWqk1qkMQoGabcqq6qZtbKEx99ZR+HOA/ROTeS+Cf25cWQ2HeNjI12eSKulYJB2r7ra\nmbtmF795Zx3Lt+4ho2sH7r6kH7ddmEPnDhrtRaQuBYNEDXdnwfpyHn93HR+sK6dbYjxfuTiXO8fm\nktwpIdLlibQaCgaJSh9v2c1v313P7E920ikhli+N6cPUcX3JSOoY6dJEIk7BIFFt7Y79/O7ddcxc\nXkxcTAw35Wdz7/j+5HTvFOnSRCJGwSACbC4/yO/nb2B6wTaq3Ll2eBZTx/VjWHa3SJcm0uIUDCI1\n7Nx3hD/M38BfFm/h4LEqLshN4a6L+zJpSKZOdZWooWAQqce+I8d5uWAbf/5wI1srDtMrOZE7x+Zy\n8wW96ZaoIb+lfVMwiDSgqtqZ8+lOnn5/I4s2VtApIZabRmVz58V96ZvWOdLliTQLBYNIE63avpc/\nfbCJmcu3U1ntXDYog7vG9WVs/+4atE/aFQWDyGnatf8Izy3cwvMLN1N+8BiDMrty17hcJo/opSuq\npV1QMIicoSPHq5i5vJin39/Imh37Se2cwJcuzOFLY/roeghp0xQMImFydxZsKOfp9zcxd81O4mKM\na4f35PYLcxjVJ0WHmaTNaWowaEAZkVMwM8b2T2Ns/zQ2lR3kzx9uYvqSbbz68XYGZXbltgtz+PzI\nXiR11NlM0r5oj0HkNBw8Wsnflxfz/KItrNy+l8T4WK47rye3j8lheHZy4x8gEkE6lCTSzFZs28ML\ni7YwY1kxh49XcW6vJG6/sA/XnddTo7tKq6RgEGkh+44cZ8bH23l+0RbW7NhPlw5xXH9+T24b3Ych\nPZMiXZ7ISQoGkRbm7izdspvnF23h9RUlHKus5vycZG6/sA/XDs/SKa8ScQoGkQjac+gY05ds44XF\nW9hQepCkjnHcOCqb20bnMDCza6TLkyilYBBpBdydhRsqeGHxFt5cVcLxKmdE72Ruys/m2uE9NT6T\ntCgFg0grU3bgKK8u3c7LS7ZSuPMAHeJiuHJoD74wKpuLB6QRG6PrIqR5KRhEWil3Z+X2vUxfso0Z\ny4rZe/g4Wd06csPIXtw4Mpt+6V0iXaK0UwoGkTbgyPEq5n66i5eXbGV+YSnVDvl9UvjCqGyuGZ5F\nV108J2dRiwWDmaUCLwK5wCbgZnffXadNH+AVIBaIBx5z9yeCZaOAPwOJwD+Ab3kjRSkYpD3aue8I\nryzdzvQlW1lfepCO8TF89twsbhqVzZh+3YnRoSYJU0sGwy+ACnd/yMweBFLc/Yd12iQE6zpqZl2A\nVcBYdy82s8XAt4CFhILhUXd/o6F1KhikPXN3Pt66h+lLtvH35cXsP1JJr+REbhyVzRdGZuu+1XLG\nWjIY1gKXunuJmWUB77r7oAbadwc+BsYADrzj7oODZbcGn3VvQ+tUMEi0OHK8irdW72D6km28v64M\ndxiZk8zkEb24ZngWaV06RLpEaUNachC9THcvAQjCIeMUBfUGZgEDgH8L9hbygW01mm0Dep2FmkTa\nhY7xsUwe0YvJI3pRvOcwM5YVM2PZdv7vzNX89PVPGDcgjckjenLF0B500TAccpY06V+Smc0BetSz\n6EdNXZG7bwWGm1lP4DUzmw7Ud9C03l0YM7sHuAcgJyenqasVaTd6JifytUv787VL+7N2x35mLNvO\njGXFfPel5XSMX8nl52QyeUQvJuSlkxAXE+lypQ1r8UNJwXv+RGjv4QN0KEnkjLk7SzbvZsayYmat\nLKHi4DG6JcZz9bAeTB7Ri9G5qeq0lpNa8lDSTGAK8FDwPKOeYrKBcnc/bGYpwMXAr4Iw2W9mY4BF\nwB3AY2ehJpGoYGbk56aSn5vKTz43hPfXlTFzWTEzlhXzl8Vb6ZHUketG9OS683oytGeSbi4kTXI2\n9hi6Ay8BOcAW4CZ3rwj6D+5z96lmNgn4JaHDRAb8xt2fDN6fz/+ervoG8IBOVxUJz6Fjlcz5dBcz\nl23n3bWlVFY7AzK6cN15PblmeBb9dRFdVNIFbiICwO6Dx3hj1Q5eW7adxRsrABiU2ZWrh2VxzfAe\nDMjQoH7RQsEgIv9ix94jvLGqhDdW7uCjzRW4Q15mF64elsXVw7LI08iv7ZqCQUQatHPfEd5ctYNZ\nK0v4aFMoJAZkhELimmFZ5GV2UZ9EO6NgEJEm27X/CG8FIbF4YwXVDv3TO5/ckxjco6tCoh1QMIjI\nGSndf5Q3V+/gjZUlLNxQTrVDv7RQSHx2WA+GZOnsprZKwSAiYSs7cJS3Vu/gHytLWLA+FBLZKYlM\nGpLJFUN6cEFuCnGxupiurVAwiMhZVX7gKHM+3cnsT3Yyv6iMY5XVJHeKZ+LgTK4Ymsn4gekkJui+\n1q2ZgkFEms3Bo5W8V1TK26t3MnfNLvYePk7H+BguGZjOpCGZTBycQXcN8NfqtOSVzyISZTp3iOOq\nc7O46twsjldV89HGCt7+ZCdvr97B7E92EmOQn5vKFcEhJw0V3rZoj0FEzhp3Z3XxPt5evYO3P9nJ\nmh37ARjcoytXDMlk0pAenNtLndeRokNJIhJxW8oP8fYnoZAo2BQ6DTYzqQOfGZTBZYMzuHhAGp01\nXHiLUTCISKtSfuAo/1yzi3fW7mJ+YRkHjlaSEBvDmP7duWxQOpcNztQhp2amYBCRVutYZTUFmyr4\n55pd/HPNLjaUHQRCV15PHJzBZwZnMKpPCvE6FfasUjCISJuxsexgaG9izS4WbSzneJWT1DGO8Xnp\nTDwngwl5GaR2Toh0mW2egkFE2qT9R47zwboy5n66i3fWllJ24CgxBufnpHDZ4Awm5KUzJCtJNyA6\nAwoGEWnzqqudldv3njzktHL7XgDSuiQwfmA64/PSuWRgmq6ZaCIFg4i0O7v2H+H9ojLmFZbyXlEZ\nFQePYQbDenVj/MB0JgxK5/zeyRqm4xQUDCLSrlVXO6uK9zJvbSnzCkv5eOseqqqdrh3juLh/GhMG\nhfYoeiUnRrrUVkPBICJRZe/h43y4roz5RaXMW1tK8d4jQOhMpwl5oZC4sG8qHeOjdzwnBYOIRC13\nZ92uA8wrDO1NLNpYwbHKajrExXBBbioXD0hj3IA0hvaMrk5sBYOISODwsSoWbSxnfmEZH64vOzlU\nR3KneC7un3YyKNr7BXYaRE9EJJCYEMulgzK4dFAGEOrEXrC+nPeLynh/XRmzVpYA0Ds1kXEDQkEx\ntn9a1F47oT0GEYlq7s7GsoN8sC4UEh+uL2f/kUoAhvZMOhkUF+Smtvn7TehQkojIGaisqmZV8b5Q\nUBSVsWTzbo5VVZMQG8PIPslc1C+Ni/p357ze3egQ17aCokWCwcxSgReBXGATcLO7767Tpg/wChAL\nxAOPufsTZtYJeBnoD1QBf3f3B5uyXgWDiLSUw8eq+GhTxck9ik9K9uEOHeNjGNUnhYv6deei/t0Z\n1iuZhLjWff1ESwXDL4AKd3/IzB4EUtz9h3XaJATrOWpmXYBVwFhgD3Chu78TtJkL/Je7v9HYehUM\nIhIpew4dY/HGChZsKGfB+vKTHdmJ8bHk56ZwUf/uXNSvO8N6dWt1F9q1VOfzZODSYHoa8C5QKxjc\n/ViNlx2AmGD+IeCdE23MbCmQHWY9IiLNKrlTAlcM7cEVQ3sAUHHwGIs3hkJiwYZyfvHmWgA6J8Ry\nQd/Uk3sUQ3t2I7aNnBobbjBkunsJgLuXmFlGfY3MrDcwCxgA/Ju7F9dZngx8Dvh/YdYjItKiUjsn\nnLzNKUDZgaMs2lDBgg1lLFhfzrtrSwHo2iGO0X1TTz7O7dWt1Q4r3uihJDObA/SoZ9GPgGnunlyj\n7W53T2ngs3oCrwGfc/edwbw44O/AW+7+6wbeew9wD0BOTs6ozZs3N1i3iEhrsGvfERZurGDB+nIW\nbSg/ee+JxPhYRvVJ4YLcUFCcn5Pc7Fdlt1Qfw1rg0mBvIQt4190HNfKePwGz3H168Ppp4IC7f7Op\n61Ufg4i0Vbv2H6Fg024Wb6zq4deWAAAGfUlEQVRg0cYK1uwIdWbHxxrnZSczum8qF/RNJb9PCl07\nxp/VdbdUMDwMlNfofE519x/UaZMdtDlsZinAIuBGd19pZj8DzgFucvfqpq5XwSAi7cXew8dZsjkU\nEos3VrBy214qq50YgyE9kxid2z0UFrkpYQ8v3lLB0B14CcgBthD6gq8ws3zgPnefamaTgF8CDhjw\nG3d/MgiMrcAa4Gjwkb9x96caW6+CQUTaq0PHKvl4yx4WB0GxdMtujlaG/m4ekNGF390+koGZXc/o\ns1vkrCR3Lwcm1jO/AJgaTM8GhtfTZhuhoBARkUCnhDguDq62htD9sVdu38PijbtZvLGcHt06NnsN\nGitJRKQVS4iLYVSfVEb1SeVrl/ZvkXW2znOlREQkYhQMIiJSi4JBRERqUTCIiEgtCgYREalFwSAi\nIrUoGEREpBYFg4iI1NImb+1pZqXAmQ6vmgaUncVyzjbVFx7VFx7VF57WXl8fd09vrFGbDIZwmFlB\nU8YKiRTVFx7VFx7VF57WXl9T6VCSiIjUomAQEZFaojEYnox0AY1QfeFRfeFRfeFp7fU1SdT1MYiI\nSMOicY9BREQaEFXBYGZXmdlaM1sX3Iq0pdff28zeMbNPzWy1mX0rmP8fZrbdzJYFj6trvOffg3rX\nmtmVLVTnJjNbGdRSEMxLNbPZZlYUPKcE883MHg1qXGFmI5uxrkE1ttEyM9tnZt+O9PYzs6fNbJeZ\nraox77S3l5lNCdoXmdmUZq7vYTNbE9TwqpklB/NzzexwjW35RI33jAr+XawLfoazcqOtU9R32r/T\n5vr/fYr6XqxR2yYzWxbMb/Ht1yzcPSoeQCywHugHJADLgSEtXEMWMDKY7goUAkOA/wC+X0/7IUGd\nHYC+Qf2xLVDnJiCtzrxfAA8G0w8CPw+mrwbeIHQ3vjHAohb8fe4A+kR6+wHjgZHAqjPdXkAqsCF4\nTgmmU5qxviuAuGD65zXqy63Zrs7nLAYuCmp/A/hsM9Z3Wr/T5vz/XV99dZb/EvhJpLZfczyiaY9h\nNLDO3Te4+zHgr8DklizA3UvcfWkwvR/4FOjVwFsmA39196PuvhFYR+jniITJwLRgehpwfY35z3jI\nQiDZzLJaoJ6JwHp3b+hCxxbZfu4+H6ioZ92ns72uBGa7e4W77wZmA1c1V33u/ra7VwYvFwLZDX1G\nUGOSuy/w0LfcMzV+prNeXwNO9Ttttv/fDdUX/NV/M/CXhj6jObdfc4imYOgFbK3xehsNfyk3KzPL\nBc4HFgWz7g92658+cdiByNXswNtmtsTM7gnmZbp7CYQCDsiIcI1fpPZ/xta0/eD0t1cka72L0F+w\nJ/Q1s4/NbJ6ZXRLM6xXU1JL1nc7vNFLb7xJgp7sX1ZjXWrbfGYumYKjveF5ETskysy7A34Bvu/s+\n4HdAf2AEUEJo1xQiV/PF7j4S+CzwDTMb30DbFq/RzBKA64CXg1mtbfs15FQ1RaRWM/sRUAk8H8wq\nAXLc/Xzgu8ALZpYUgfpO93caqd/1rdT+A6W1bL+wRFMwbAN613idDRS3dBFmFk8oFJ5391cA3H2n\nu1e5ezXwB/73cEdEanb34uB5F/BqUM/OE4eIguddEazxs8BSd98Z1Nmqtl/gdLdXi9cadHBfC9we\nHN4gOERTHkwvIXTcPi+or+bhpmat7wx+p5HYfnHADcCLNepuFdsvXNEUDB8BA82sb/AX5xeBmS1Z\nQHA88o/Ap+7+qxrzax6T/zxw4uyHmcAXzayDmfUFBhLqwGrOGjubWdcT04Q6KVcFtZw4U2YKMKNG\njXcEZ9uMAfaeOITSjGr9ldaatl8Np7u93gKuMLOU4LDJFcG8ZmFmVwE/BK5z90M15qebWWww3Y/Q\nNtsQ1LjfzMYE/47vqPEzNUd9p/s7jcT/78uBNe5+8hBRa9l+YYt073dLPgidEVJIKMV/FIH1jyO0\n+7gCWBY8rgaeBVYG82cCWTXe86Og3rW0wFkMhM7qWB48Vp/YTkB3YC5QFDynBvMNeDyocSWQ38z1\ndQLKgW415kV0+xEKqRLgOKG/DL96JtuL0LH+dcHjK81c3zpCx+RP/Dt8Imh7Y/B7Xw4sBT5X43Py\nCX1Brwd+Q3CBbDPVd9q/0+b6/11ffcH8PwP31Wnb4tuvOR668llERGqJpkNJIiLSBAoGERGpRcEg\nIiK1KBhERKQWBYOIiNSiYBARkVoUDCIiUouCQUREavn/BhsSFLtm0cEAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x258f5fb6358>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(softmax.training_costs[0], softmax.training_costs[1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now with the model implemented, we do some hyperparameter coarse-fine tuning. We will do so by generating parameter values for the learning_rate, lambda, number of features, and number of grams to use. For each specified set of parameters, we will train the model and compute the result on the dev set. We will select the parameters that perform the best on the dev set.\n",
    "Note that we begin with the defaults num_grams = 3, num_features = 1800, learning_rate = 0.3, lambda = 0.03"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "defaults = [3, 1800, 0.3, 0.09]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data, train_data_labels, train_sentiment_labels = train[0], labeled_data_finegrained(train[1]), train[1]\n",
    "dev_data, dev_data_labels, dev_sentiment_labels = dev[0], labeled_data_finegrained(dev[1]), dev[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building features.\n",
      "Features built.\n",
      "Now converting documents to input matrix.\n",
      "8117 training examples\n",
      "Initiating batch gradient descent with  2000  iterations.\n",
      "here\n",
      "Building features.\n",
      "Features built.\n",
      "Now converting documents to input matrix.\n",
      "8117 training examples\n",
      "Initiating batch gradient descent with  2000  iterations.\n",
      "Classifier with  3 -grams,  1500  features, learning rate  0.269877417429  and regularization parameter  0.00514445089244  achieved error rate of  0.3357575757575758\n",
      "Building features.\n",
      "Features built.\n",
      "Now converting documents to input matrix.\n",
      "8117 training examples\n",
      "Initiating batch gradient descent with  2000  iterations.\n",
      "Classifier with  3 -grams,  1500  features, learning rate  0.432757571018  and regularization parameter  0.0129565498727  achieved error rate of  0.33818181818181814\n",
      "Building features.\n",
      "Features built.\n",
      "Now converting documents to input matrix.\n",
      "8117 training examples\n",
      "Initiating batch gradient descent with  2000  iterations.\n",
      "Classifier with  3 -grams,  1500  features, learning rate  0.100676230475  and regularization parameter  0.0198273956504  achieved error rate of  0.33333333333333337\n",
      "Building features.\n",
      "Features built.\n",
      "Now converting documents to input matrix.\n",
      "8117 training examples\n",
      "Initiating batch gradient descent with  2000  iterations.\n",
      "Classifier with  3 -grams,  1500  features, learning rate  0.13199008882  and regularization parameter  0.0217720704689  achieved error rate of  0.3357575757575758\n",
      "Building features.\n",
      "Features built.\n",
      "Now converting documents to input matrix.\n",
      "8117 training examples\n",
      "Initiating batch gradient descent with  2000  iterations.\n",
      "Classifier with  3 -grams,  1500  features, learning rate  0.408276349211  and regularization parameter  0.0108647428385  achieved error rate of  0.33333333333333337\n",
      "Building features.\n",
      "Features built.\n",
      "Now converting documents to input matrix.\n",
      "8117 training examples\n",
      "Initiating batch gradient descent with  2000  iterations.\n",
      "Classifier with  3 -grams,  1500  features, learning rate  0.13443790877  and regularization parameter  0.0186383881439  achieved error rate of  0.3345454545454546\n",
      "Building features.\n",
      "Features built.\n",
      "Now converting documents to input matrix.\n",
      "8117 training examples\n",
      "Initiating batch gradient descent with  2000  iterations.\n",
      "Classifier with  3 -grams,  1500  features, learning rate  0.169539107191  and regularization parameter  0.0140363522924  achieved error rate of  0.33090909090909093\n",
      "Building features.\n",
      "Features built.\n",
      "Now converting documents to input matrix.\n",
      "8117 training examples\n",
      "Initiating batch gradient descent with  2000  iterations.\n",
      "Classifier with  3 -grams,  1500  features, learning rate  0.147662551374  and regularization parameter  0.0173497203319  achieved error rate of  0.33333333333333337\n",
      "Building features.\n",
      "Features built.\n",
      "Now converting documents to input matrix.\n",
      "8117 training examples\n",
      "Initiating batch gradient descent with  2000  iterations.\n",
      "Classifier with  3 -grams,  1500  features, learning rate  0.168553874795  and regularization parameter  0.0390808460202  achieved error rate of  0.33333333333333337\n",
      "Building features.\n",
      "Features built.\n",
      "Now converting documents to input matrix.\n",
      "8117 training examples\n",
      "Initiating batch gradient descent with  2000  iterations.\n",
      "Classifier with  3 -grams,  1500  features, learning rate  0.385646714572  and regularization parameter  0.013687881218  achieved error rate of  0.33333333333333337\n",
      "Building features.\n",
      "Features built.\n",
      "Now converting documents to input matrix.\n",
      "8117 training examples\n",
      "Initiating batch gradient descent with  2000  iterations.\n",
      "Classifier with  3 -grams,  1500  features, learning rate  0.447736204547  and regularization parameter  0.0311489998408  achieved error rate of  0.33333333333333337\n",
      "Building features.\n",
      "Features built.\n",
      "Now converting documents to input matrix.\n",
      "8117 training examples\n",
      "Initiating batch gradient descent with  2000  iterations.\n",
      "Classifier with  3 -grams,  1500  features, learning rate  0.439983723633  and regularization parameter  0.0212758269259  achieved error rate of  0.33939393939393936\n",
      "Building features.\n",
      "Features built.\n",
      "Now converting documents to input matrix.\n",
      "8117 training examples\n",
      "Initiating batch gradient descent with  2000  iterations.\n",
      "Classifier with  3 -grams,  1500  features, learning rate  0.712754817923  and regularization parameter  0.0056392522108  achieved error rate of  0.3418181818181818\n",
      "Building features.\n",
      "Features built.\n",
      "Now converting documents to input matrix.\n",
      "8117 training examples\n",
      "Initiating batch gradient descent with  2000  iterations.\n",
      "Classifier with  3 -grams,  1500  features, learning rate  0.195502210763  and regularization parameter  0.0109793987832  achieved error rate of  0.3296969696969697\n",
      "Building features.\n",
      "Features built.\n",
      "Now converting documents to input matrix.\n",
      "8117 training examples\n",
      "Initiating batch gradient descent with  2000  iterations.\n",
      "Classifier with  3 -grams,  1500  features, learning rate  0.670258026821  and regularization parameter  0.0141111340552  achieved error rate of  0.33939393939393936\n",
      "Building features.\n",
      "Features built.\n",
      "Now converting documents to input matrix.\n",
      "8117 training examples\n",
      "Initiating batch gradient descent with  2000  iterations.\n",
      "Classifier with  3 -grams,  1500  features, learning rate  0.261432780461  and regularization parameter  0.0186693523901  achieved error rate of  0.3345454545454546\n",
      "Building features.\n",
      "Features built.\n",
      "Now converting documents to input matrix.\n",
      "8117 training examples\n",
      "Initiating batch gradient descent with  2000  iterations.\n",
      "Classifier with  3 -grams,  1500  features, learning rate  0.131911655617  and regularization parameter  0.0107323912477  achieved error rate of  0.33333333333333337\n",
      "Building features.\n",
      "Features built.\n",
      "Now converting documents to input matrix.\n",
      "8117 training examples\n",
      "Initiating batch gradient descent with  2000  iterations.\n",
      "Classifier with  3 -grams,  1500  features, learning rate  0.123675953559  and regularization parameter  0.0232253313714  achieved error rate of  0.3357575757575758\n",
      "Building features.\n",
      "Features built.\n",
      "Now converting documents to input matrix.\n",
      "8117 training examples\n",
      "Initiating batch gradient descent with  2000  iterations.\n",
      "Classifier with  3 -grams,  1500  features, learning rate  0.466961005349  and regularization parameter  0.0190979505355  achieved error rate of  0.3345454545454546\n",
      "Building features.\n",
      "Features built.\n",
      "Now converting documents to input matrix.\n",
      "8117 training examples\n",
      "Initiating batch gradient descent with  2000  iterations.\n",
      "Classifier with  3 -grams,  1500  features, learning rate  0.542187352627  and regularization parameter  0.0111808210559  achieved error rate of  0.3406060606060606\n",
      "Building features.\n",
      "Features built.\n",
      "Now converting documents to input matrix.\n",
      "8117 training examples\n",
      "Initiating batch gradient descent with  2000  iterations.\n",
      "Classifier with  3 -grams,  1800  features, learning rate  0.243945086119  and regularization parameter  0.0231344692297  achieved error rate of  0.3296969696969697\n",
      "Building features.\n",
      "Features built.\n",
      "Now converting documents to input matrix.\n",
      "8117 training examples\n",
      "Initiating batch gradient descent with  2000  iterations.\n",
      "Classifier with  3 -grams,  1800  features, learning rate  0.106301556976  and regularization parameter  0.00673746652019  achieved error rate of  0.33333333333333337\n",
      "Building features.\n",
      "Features built.\n",
      "Now converting documents to input matrix.\n",
      "8117 training examples\n",
      "Initiating batch gradient descent with  2000  iterations.\n",
      "Classifier with  3 -grams,  1800  features, learning rate  0.18544377826  and regularization parameter  0.0112321396989  achieved error rate of  0.32484848484848483\n",
      "New best.\n",
      "Building features.\n",
      "Features built.\n",
      "Now converting documents to input matrix.\n",
      "8117 training examples\n",
      "Initiating batch gradient descent with  2000  iterations.\n",
      "Classifier with  3 -grams,  1800  features, learning rate  0.244199796471  and regularization parameter  0.0428746414264  achieved error rate of  0.3284848484848485\n",
      "Building features.\n",
      "Features built.\n",
      "Now converting documents to input matrix.\n",
      "8117 training examples\n",
      "Initiating batch gradient descent with  2000  iterations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier with  3 -grams,  1800  features, learning rate  0.16559252148  and regularization parameter  0.0123989285277  achieved error rate of  0.32484848484848483\n",
      "Building features.\n",
      "Features built.\n",
      "Now converting documents to input matrix.\n",
      "8117 training examples\n",
      "Initiating batch gradient descent with  2000  iterations.\n",
      "Classifier with  3 -grams,  1800  features, learning rate  0.207115587686  and regularization parameter  0.0443552559341  achieved error rate of  0.32727272727272727\n",
      "Building features.\n",
      "Features built.\n",
      "Now converting documents to input matrix.\n",
      "8117 training examples\n",
      "Initiating batch gradient descent with  2000  iterations.\n",
      "Classifier with  3 -grams,  1800  features, learning rate  0.79310580708  and regularization parameter  0.0108823010246  achieved error rate of  0.3418181818181818\n",
      "Building features.\n",
      "Features built.\n",
      "Now converting documents to input matrix.\n",
      "8117 training examples\n",
      "Initiating batch gradient descent with  2000  iterations.\n",
      "Classifier with  3 -grams,  1800  features, learning rate  0.565671945235  and regularization parameter  0.00526521310429  achieved error rate of  0.3369696969696969\n",
      "Building features.\n",
      "Features built.\n",
      "Now converting documents to input matrix.\n",
      "8117 training examples\n",
      "Initiating batch gradient descent with  2000  iterations.\n",
      "Classifier with  3 -grams,  1800  features, learning rate  0.297543479622  and regularization parameter  0.0240466545612  achieved error rate of  0.3296969696969697\n",
      "Building features.\n",
      "Features built.\n",
      "Now converting documents to input matrix.\n",
      "8117 training examples\n",
      "Initiating batch gradient descent with  2000  iterations.\n",
      "Classifier with  3 -grams,  1800  features, learning rate  0.295031691801  and regularization parameter  0.042088918281  achieved error rate of  0.3284848484848485\n",
      "Building features.\n",
      "Features built.\n",
      "Now converting documents to input matrix.\n",
      "8117 training examples\n",
      "Initiating batch gradient descent with  2000  iterations.\n",
      "Classifier with  3 -grams,  1800  features, learning rate  0.391438499471  and regularization parameter  0.0383591754817  achieved error rate of  0.3369696969696969\n",
      "Building features.\n",
      "Features built.\n",
      "Now converting documents to input matrix.\n",
      "8117 training examples\n",
      "Initiating batch gradient descent with  2000  iterations.\n",
      "Classifier with  3 -grams,  1800  features, learning rate  0.339809219234  and regularization parameter  0.00986298180722  achieved error rate of  0.33090909090909093\n",
      "Building features.\n",
      "Features built.\n",
      "Now converting documents to input matrix.\n",
      "8117 training examples\n",
      "Initiating batch gradient descent with  2000  iterations.\n",
      "Classifier with  3 -grams,  1800  features, learning rate  0.656256328726  and regularization parameter  0.00941787627427  achieved error rate of  0.3406060606060606\n",
      "Building features.\n",
      "Features built.\n",
      "Now converting documents to input matrix.\n",
      "8117 training examples\n",
      "Initiating batch gradient descent with  2000  iterations.\n",
      "Classifier with  3 -grams,  1800  features, learning rate  0.298059661462  and regularization parameter  0.016848396432  achieved error rate of  0.3284848484848485\n",
      "Building features.\n",
      "Features built.\n",
      "Now converting documents to input matrix.\n",
      "8117 training examples\n",
      "Initiating batch gradient descent with  2000  iterations.\n",
      "Classifier with  3 -grams,  1800  features, learning rate  0.104621940634  and regularization parameter  0.00877613088176  achieved error rate of  0.33333333333333337\n",
      "Building features.\n",
      "Features built.\n",
      "Now converting documents to input matrix.\n",
      "8117 training examples\n",
      "Initiating batch gradient descent with  2000  iterations.\n",
      "Classifier with  3 -grams,  1800  features, learning rate  0.417355915017  and regularization parameter  0.0421987455498  achieved error rate of  0.3406060606060606\n",
      "Building features.\n",
      "Features built.\n",
      "Now converting documents to input matrix.\n",
      "8117 training examples\n",
      "Initiating batch gradient descent with  2000  iterations.\n",
      "Classifier with  3 -grams,  1800  features, learning rate  0.950850946078  and regularization parameter  0.00595640609876  achieved error rate of  0.3490909090909091\n",
      "Building features.\n",
      "Features built.\n",
      "Now converting documents to input matrix.\n",
      "8117 training examples\n",
      "Initiating batch gradient descent with  2000  iterations.\n",
      "Classifier with  3 -grams,  1800  features, learning rate  0.125153491709  and regularization parameter  0.028600730311  achieved error rate of  0.3345454545454546\n",
      "Building features.\n",
      "Features built.\n",
      "Now converting documents to input matrix.\n",
      "8117 training examples\n",
      "Initiating batch gradient descent with  2000  iterations.\n",
      "Classifier with  3 -grams,  1800  features, learning rate  0.763054007928  and regularization parameter  0.0284271436987  achieved error rate of  0.3418181818181818\n",
      "Building features.\n",
      "Features built.\n",
      "Now converting documents to input matrix.\n",
      "8117 training examples\n",
      "Initiating batch gradient descent with  2000  iterations.\n",
      "Classifier with  3 -grams,  1800  features, learning rate  0.306845002322  and regularization parameter  0.00737221543022  achieved error rate of  0.33090909090909093\n",
      "Building features.\n",
      "Features built.\n",
      "Now converting documents to input matrix.\n",
      "8117 training examples\n",
      "Initiating batch gradient descent with  2000  iterations.\n",
      "Classifier with  3 -grams,  2100  features, learning rate  0.996786607036  and regularization parameter  0.006405591049  achieved error rate of  0.35030303030303034\n",
      "Building features.\n",
      "Features built.\n",
      "Now converting documents to input matrix.\n",
      "8117 training examples\n",
      "Initiating batch gradient descent with  2000  iterations.\n",
      "Classifier with  3 -grams,  2100  features, learning rate  0.142424334429  and regularization parameter  0.0183931863293  achieved error rate of  0.32606060606060605\n",
      "Building features.\n",
      "Features built.\n",
      "Now converting documents to input matrix.\n",
      "8117 training examples\n",
      "Initiating batch gradient descent with  2000  iterations.\n",
      "Classifier with  3 -grams,  2100  features, learning rate  0.137640191335  and regularization parameter  0.0266175145875  achieved error rate of  0.3284848484848485\n",
      "Building features.\n",
      "Features built.\n",
      "Now converting documents to input matrix.\n",
      "8117 training examples\n",
      "Initiating batch gradient descent with  2000  iterations.\n",
      "Classifier with  3 -grams,  2100  features, learning rate  0.455064519184  and regularization parameter  0.0478023768069  achieved error rate of  0.3345454545454546\n",
      "Building features.\n",
      "Features built.\n",
      "Now converting documents to input matrix.\n",
      "8117 training examples\n",
      "Initiating batch gradient descent with  2000  iterations.\n",
      "Classifier with  3 -grams,  2100  features, learning rate  0.215168829505  and regularization parameter  0.0444784351424  achieved error rate of  0.32727272727272727\n",
      "Building features.\n",
      "Features built.\n",
      "Now converting documents to input matrix.\n",
      "8117 training examples\n",
      "Initiating batch gradient descent with  2000  iterations.\n",
      "Classifier with  3 -grams,  2100  features, learning rate  0.260052578979  and regularization parameter  0.0356016101985  achieved error rate of  0.3284848484848485\n",
      "Building features.\n",
      "Features built.\n",
      "Now converting documents to input matrix.\n",
      "8117 training examples\n",
      "Initiating batch gradient descent with  2000  iterations.\n",
      "Classifier with  3 -grams,  2100  features, learning rate  0.40620423474  and regularization parameter  0.0363716432131  achieved error rate of  0.3345454545454546\n",
      "Building features.\n",
      "Features built.\n",
      "Now converting documents to input matrix.\n",
      "8117 training examples\n",
      "Initiating batch gradient descent with  2000  iterations.\n",
      "Classifier with  3 -grams,  2100  features, learning rate  0.226257287115  and regularization parameter  0.0340931124327  achieved error rate of  0.32727272727272727\n",
      "Building features.\n",
      "Features built.\n",
      "Now converting documents to input matrix.\n",
      "8117 training examples\n",
      "Initiating batch gradient descent with  2000  iterations.\n",
      "Classifier with  3 -grams,  2100  features, learning rate  0.188230803405  and regularization parameter  0.039182199104  achieved error rate of  0.3236363636363636\n",
      "New best.\n",
      "Building features.\n",
      "Features built.\n",
      "Now converting documents to input matrix.\n",
      "8117 training examples\n",
      "Initiating batch gradient descent with  2000  iterations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier with  3 -grams,  2100  features, learning rate  0.387541969847  and regularization parameter  0.0107786814154  achieved error rate of  0.33090909090909093\n",
      "Building features.\n",
      "Features built.\n",
      "Now converting documents to input matrix.\n",
      "8117 training examples\n",
      "Initiating batch gradient descent with  2000  iterations.\n",
      "Classifier with  3 -grams,  2100  features, learning rate  0.20562144925  and regularization parameter  0.0173233757511  achieved error rate of  0.32484848484848483\n",
      "Building features.\n",
      "Features built.\n",
      "Now converting documents to input matrix.\n",
      "8117 training examples\n",
      "Initiating batch gradient descent with  2000  iterations.\n",
      "Classifier with  3 -grams,  2100  features, learning rate  0.728819254178  and regularization parameter  0.00568097725606  achieved error rate of  0.3466666666666667\n",
      "Building features.\n",
      "Features built.\n",
      "Now converting documents to input matrix.\n",
      "8117 training examples\n",
      "Initiating batch gradient descent with  2000  iterations.\n",
      "Classifier with  3 -grams,  2100  features, learning rate  0.597032060764  and regularization parameter  0.0156576681119  achieved error rate of  0.3406060606060606\n",
      "Building features.\n",
      "Features built.\n",
      "Now converting documents to input matrix.\n",
      "8117 training examples\n",
      "Initiating batch gradient descent with  2000  iterations.\n",
      "Classifier with  3 -grams,  2100  features, learning rate  0.125209951304  and regularization parameter  0.0450592564412  achieved error rate of  0.3296969696969697\n",
      "Building features.\n",
      "Features built.\n",
      "Now converting documents to input matrix.\n",
      "8117 training examples\n",
      "Initiating batch gradient descent with  2000  iterations.\n",
      "Classifier with  3 -grams,  2100  features, learning rate  0.700840377384  and regularization parameter  0.00815325464703  achieved error rate of  0.3466666666666667\n",
      "Building features.\n",
      "Features built.\n",
      "Now converting documents to input matrix.\n",
      "8117 training examples\n",
      "Initiating batch gradient descent with  2000  iterations.\n",
      "Classifier with  3 -grams,  2100  features, learning rate  0.150599175353  and regularization parameter  0.023302461553  achieved error rate of  0.3236363636363636\n",
      "Building features.\n",
      "Features built.\n",
      "Now converting documents to input matrix.\n",
      "8117 training examples\n",
      "Initiating batch gradient descent with  2000  iterations.\n",
      "Classifier with  3 -grams,  2100  features, learning rate  0.160133483864  and regularization parameter  0.0242060534808  achieved error rate of  0.3224242424242424\n",
      "New best.\n",
      "Building features.\n",
      "Features built.\n",
      "Now converting documents to input matrix.\n",
      "8117 training examples\n",
      "Initiating batch gradient descent with  2000  iterations.\n",
      "Classifier with  3 -grams,  2100  features, learning rate  0.239904239218  and regularization parameter  0.00619733891397  achieved error rate of  0.3284848484848485\n",
      "Building features.\n",
      "Features built.\n",
      "Now converting documents to input matrix.\n",
      "8117 training examples\n",
      "Initiating batch gradient descent with  2000  iterations.\n",
      "Classifier with  3 -grams,  2100  features, learning rate  0.319843673688  and regularization parameter  0.0184785687187  achieved error rate of  0.3236363636363636\n",
      "Building features.\n",
      "Features built.\n",
      "Now converting documents to input matrix.\n",
      "8117 training examples\n",
      "Initiating batch gradient descent with  2000  iterations.\n",
      "Classifier with  3 -grams,  2100  features, learning rate  0.330877967302  and regularization parameter  0.0316651771  achieved error rate of  0.32484848484848483\n",
      "Done coarse tuning. Best parameters are  [3, 2100, 0.16013348386399803, 0.024206053480780591]  with a dev error of  0.3224242424242424\n"
     ]
    }
   ],
   "source": [
    "# Establish initial results\n",
    "init = SoftmaxRegression(defaults[0], defaults[1], defaults[2], defaults[3])\n",
    "init.train([train_data, train_data_labels])\n",
    "labels_init = maxIndicesToLabels(init.test(dev_data))\n",
    "init_results = polarity_error_nb(labels_init, dev_sentiment_labels)\n",
    "\n",
    "best_error = init_results\n",
    "# best_error = 0.3436249285305889\n",
    "best_params = [defaults]\n",
    "summaries = [[[defaults], init_results]]\n",
    "print(\"Initial results are \", summaries)\n",
    "from random import *\n",
    "\n",
    "# Do a coarse search. \n",
    "for n in range(3, 4): # run through possible number of n_grams\n",
    "    for i in range(1, 4):\n",
    "        num_feat = 1200 + i * 300\n",
    "        for j in range(20):\n",
    "            t = -1 * uniform(0,1)\n",
    "            alpha = np.power(10, t)\n",
    "            r = -1 * uniform(2,3)\n",
    "            Lambda = 5 * np.power(10, r)\n",
    "            tuning_run = SoftmaxRegression(n, num_feat, alpha, Lambda)\n",
    "            tuning_run.train([train_data, train_data_labels])\n",
    "            labels = maxIndicesToLabels(tuning_run.test(dev_data))\n",
    "            result = polarity_error_nb(labels, dev_sentiment_labels)\n",
    "            print(\"Classifier with \", n, \"-grams, \", num_feat, \" features, learning rate \", alpha,\n",
    "                  \" and regularization parameter \", Lambda, \" achieved error rate of \", result) \n",
    "            params = [n, num_feat, alpha, Lambda]\n",
    "            summaries.append([params, result])\n",
    "            if (result < best_error):\n",
    "                print(\"New best.\")\n",
    "                best_error = result\n",
    "                best_params = params\n",
    "\n",
    "print(\"Done coarse tuning. Best parameters are \", best_params, \" with a dev error of \", best_error)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[[[3, 1800, 0.3, 0.09]], 0.3284848484848485],\n",
       " [[3, 1500, 0.26987741742908922, 0.005144450892436649], 0.3357575757575758],\n",
       " [[3, 1500, 0.43275757101762063, 0.012956549872654814], 0.33818181818181814],\n",
       " [[3, 1500, 0.10067623047460032, 0.019827395650362574], 0.33333333333333337],\n",
       " [[3, 1500, 0.13199008882025237, 0.021772070468854599], 0.3357575757575758],\n",
       " [[3, 1500, 0.40827634921062306, 0.010864742838506532], 0.33333333333333337],\n",
       " [[3, 1500, 0.13443790877048473, 0.018638388143919209], 0.3345454545454546],\n",
       " [[3, 1500, 0.16953910719098658, 0.014036352292414239], 0.33090909090909093],\n",
       " [[3, 1500, 0.14766255137411835, 0.017349720331944481], 0.33333333333333337],\n",
       " [[3, 1500, 0.16855387479513353, 0.039080846020155005], 0.33333333333333337],\n",
       " [[3, 1500, 0.38564671457172733, 0.013687881217957045], 0.33333333333333337],\n",
       " [[3, 1500, 0.44773620454742308, 0.031148999840806931], 0.33333333333333337],\n",
       " [[3, 1500, 0.43998372363322974, 0.021275826925941821], 0.33939393939393936],\n",
       " [[3, 1500, 0.71275481792323936, 0.005639252210797867], 0.3418181818181818],\n",
       " [[3, 1500, 0.19550221076273877, 0.010979398783241433], 0.3296969696969697],\n",
       " [[3, 1500, 0.67025802682057889, 0.014111134055237548], 0.33939393939393936],\n",
       " [[3, 1500, 0.26143278046110058, 0.018669352390116918], 0.3345454545454546],\n",
       " [[3, 1500, 0.13191165561669418, 0.010732391247659269], 0.33333333333333337],\n",
       " [[3, 1500, 0.12367595355943184, 0.023225331371434559], 0.3357575757575758],\n",
       " [[3, 1500, 0.46696100534938201, 0.019097950535485173], 0.3345454545454546],\n",
       " [[3, 1500, 0.54218735262668349, 0.011180821055901515], 0.3406060606060606],\n",
       " [[3, 1800, 0.24394508611891774, 0.023134469229745096], 0.3296969696969697],\n",
       " [[3, 1800, 0.10630155697571203, 0.0067374665201890872], 0.33333333333333337],\n",
       " [[3, 1800, 0.1854437782598472, 0.011232139698934664], 0.32484848484848483],\n",
       " [[3, 1800, 0.24419979647120815, 0.042874641426444454], 0.3284848484848485],\n",
       " [[3, 1800, 0.16559252147971185, 0.012398928527740743], 0.32484848484848483],\n",
       " [[3, 1800, 0.20711558768612587, 0.044355255934102673], 0.32727272727272727],\n",
       " [[3, 1800, 0.79310580708033662, 0.010882301024628276], 0.3418181818181818],\n",
       " [[3, 1800, 0.56567194523491404, 0.0052652131042933824], 0.3369696969696969],\n",
       " [[3, 1800, 0.29754347962188604, 0.024046654561192975], 0.3296969696969697],\n",
       " [[3, 1800, 0.29503169180133348, 0.042088918280963859], 0.3284848484848485],\n",
       " [[3, 1800, 0.39143849947117337, 0.038359175481734009], 0.3369696969696969],\n",
       " [[3, 1800, 0.33980921923358498, 0.0098629818072155911], 0.33090909090909093],\n",
       " [[3, 1800, 0.65625632872603512, 0.0094178762742666861], 0.3406060606060606],\n",
       " [[3, 1800, 0.29805966146195578, 0.016848396432040301], 0.3284848484848485],\n",
       " [[3, 1800, 0.1046219406339385, 0.0087761308817623576], 0.33333333333333337],\n",
       " [[3, 1800, 0.4173559150174847, 0.042198745549842549], 0.3406060606060606],\n",
       " [[3, 1800, 0.95085094607830356, 0.0059564060987628096], 0.3490909090909091],\n",
       " [[3, 1800, 0.12515349170930173, 0.028600730310986838], 0.3345454545454546],\n",
       " [[3, 1800, 0.76305400792761857, 0.028427143698745955], 0.3418181818181818],\n",
       " [[3, 1800, 0.30684500232179263, 0.0073722154302230643], 0.33090909090909093],\n",
       " [[3, 2100, 0.99678660703631339, 0.0064055910489988176], 0.35030303030303034],\n",
       " [[3, 2100, 0.14242433442860886, 0.018393186329325673], 0.32606060606060605],\n",
       " [[3, 2100, 0.13764019133455993, 0.026617514587521297], 0.3284848484848485],\n",
       " [[3, 2100, 0.45506451918428681, 0.047802376806930738], 0.3345454545454546],\n",
       " [[3, 2100, 0.2151688295051507, 0.044478435142439043], 0.32727272727272727],\n",
       " [[3, 2100, 0.26005257897860967, 0.03560161019850628], 0.3284848484848485],\n",
       " [[3, 2100, 0.40620423474005485, 0.036371643213064429], 0.3345454545454546],\n",
       " [[3, 2100, 0.22625728711452978, 0.034093112432711965], 0.32727272727272727],\n",
       " [[3, 2100, 0.18823080340498544, 0.039182199104026538], 0.3236363636363636],\n",
       " [[3, 2100, 0.38754196984682693, 0.010778681415358718], 0.33090909090909093],\n",
       " [[3, 2100, 0.20562144924995496, 0.017323375751074024], 0.32484848484848483],\n",
       " [[3, 2100, 0.72881925417787863, 0.0056809772560564194], 0.3466666666666667],\n",
       " [[3, 2100, 0.59703206076382997, 0.015657668111949342], 0.3406060606060606],\n",
       " [[3, 2100, 0.12520995130375911, 0.045059256441236845], 0.3296969696969697],\n",
       " [[3, 2100, 0.7008403773836569, 0.0081532546470341593], 0.3466666666666667],\n",
       " [[3, 2100, 0.15059917535263018, 0.023302461553043365], 0.3236363636363636],\n",
       " [[3, 2100, 0.16013348386399803, 0.024206053480780591], 0.3224242424242424],\n",
       " [[3, 2100, 0.23990423921800261, 0.0061973389139710275], 0.3284848484848485],\n",
       " [[3, 2100, 0.319843673687707, 0.018478568718733143], 0.3236363636363636],\n",
       " [[3, 2100, 0.33087796730240543, 0.031665177100019995], 0.32484848484848483]]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
