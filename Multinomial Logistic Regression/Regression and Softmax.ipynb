{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.util import ngrams\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "# An implementation of Softmax Regression (aka Multinomial logistic regression) classifier. \n",
    "# train and test functions require a pair of lists (document, label), where label is in the format (0,..., 1, ...0) indicating\n",
    "# to which class the label belongs\n",
    "# initiated with n for n-gram \n",
    "\n",
    "# class computes n-grams\n",
    "\n",
    "class SoftmaxRegression:\n",
    "    \n",
    "    def __init__(self, num_grams=3, num_features=1800, learning_rate = 0.3, LAMBDA = 0.03):\n",
    "        self.num_grams = num_grams\n",
    "        self.num_features = num_features  \n",
    "        self.features_built = False\n",
    "        self.learning_rate = learning_rate\n",
    "        self.LAMBDA = LAMBDA\n",
    "        \n",
    "    def train(self, training_set):\n",
    "        print(\"Building features.\")\n",
    "        self.K = len(training_set[1][0])\n",
    "        self.grams_features = self.build_features(training_set[0]) # extends grams_features as necessary\n",
    "        print(\"Features built.\")\n",
    "        print(\"Now converting documents to input matrix.\")\n",
    "        X = self.build_input(training_set[0])\n",
    "        Y = training_set[1]\n",
    "      \n",
    "        self.theta = self.initialize_parameters()\n",
    "        \n",
    "        self.batch_gradient_descent(X, Y, 2000)\n",
    "    \n",
    "    def batch_gradient_descent(self, X, Y, num_iterations = 2000):\n",
    "        m = X.shape[0]\n",
    "        print(m, \"training examples\")\n",
    "        n = self.num_features \n",
    "        self.training_costs = [[],[]]\n",
    "        print(\"Initiating batch gradient descent with \" , num_iterations, \" iterations.\")\n",
    "        for i in range(num_iterations):\n",
    "            H = self.softmax(np.dot(X, self.theta))\n",
    "            if i % 100 == 0:\n",
    "                cost = self.cost(X, Y, H)\n",
    "               # print(\"Loss after \", i, \" iterations is \", cost)\n",
    "                self.training_costs[0].append(i)\n",
    "                self.training_costs[1].append(cost)\n",
    "            grads = self.grads(X, Y, H)\n",
    "            self.theta = self.theta - self.learning_rate * grads\n",
    "            \n",
    "    def test(self, test_set):\n",
    "        test_X = self.build_input(test_set)\n",
    "        test_Y = self.softmax(np.dot(test_X, self.theta)) \n",
    "        max_indices = np.argmax(test_Y, axis = 1)\n",
    "        return max_indices\n",
    "    \n",
    "    def build_input(self, corpus):\n",
    "        X = []\n",
    "        for document in corpus:\n",
    "            X.append(self.doc_to_gram_vec(document))\n",
    "        return np.array(X)\n",
    "    \n",
    "    def build_features(self, corpus):\n",
    "        all_grams = []\n",
    "        for n in range(1, self.num_grams):\n",
    "            ngrams = []\n",
    "            \n",
    "            for doc in corpus:\n",
    "                ngrams.extend(self.compute_grams(doc, n))\n",
    "                \n",
    "            fdist = FreqDist(ngram for ngram in ngrams)\n",
    "            for phrase in fdist.most_common(self.num_features // self.num_grams):\n",
    "                all_grams.append(phrase[0]) # add common n-gram\n",
    "        grams_dict = dict(zip(all_grams, range(len(all_grams)))) # converts into dictionary with positions\n",
    "        self.features_built = True\n",
    "        return grams_dict\n",
    "    \n",
    "    def doc_to_gram_vec(self, doc): # given document, returns vector representing all features\n",
    "        assert self.features_built\n",
    "        doc_vec = np.zeros(self.num_features) \n",
    "        doc_grams = []\n",
    "        for n in range(1, self.num_grams):\n",
    "            doc_grams.extend(self.compute_grams(doc, n))\n",
    "            \n",
    "        for gram in doc_grams:\n",
    "            if gram in self.grams_features:\n",
    "                doc_vec[self.grams_features[gram]] = 1\n",
    "        \n",
    "        return doc_vec\n",
    "        \n",
    "    def compute_grams(self, doc, num_grams):  # given a document, and selected n num_grams, computes all n_grams\n",
    "        tokens = word_tokenize(doc)\n",
    "        if num_grams == 1:\n",
    "            return tokens\n",
    "        else:\n",
    "            return ngrams(tokens, num_grams)        \n",
    "    \n",
    "    def initialize_parameters(self):\n",
    "        theta = np.random.randn(self.num_features, self.K) * 0.01\n",
    "        return theta\n",
    "    \n",
    "    def cost(self, X, Y, H): \n",
    "        m = X.shape[0]\n",
    "        return -(1/m) *(np.sum(np.multiply(Y, H)) + (self.LAMBDA / 2) * np.sum(np.power(self.theta, 2)))\n",
    "    \n",
    "    def grads(self, X, Y, H): # grads will be a matrix\n",
    "        LAMBDA = self.LAMBDA\n",
    "        grads = -np.dot(X.T, (Y-H))\n",
    "        m = X.shape[0]\n",
    "        grads = (1/m) * (grads + LAMBDA * self.theta)\n",
    "        return grads\n",
    "    \n",
    "    def update_parameters(self, grads, learning_rate):\n",
    "        self.theta = self.theta - (learning_rate * grads)\n",
    "        \n",
    "    def sigmoid(self, x):\n",
    "        z = np.exp(x)\n",
    "        z = z / (z+1)\n",
    "        return z\n",
    "    \n",
    "    def predict(self, x):\n",
    "        probs = softmax(np.dot(self.theta.T, x))\n",
    "        return argmax(probs)\n",
    "    \n",
    "    def softmax(self, Z): # Given matrix Z, returns softmax treating each row as a vector\n",
    "        Z = np.exp(Z)\n",
    "        denoms = np.sum(Z, axis = 1)\n",
    "        denoms = denoms.reshape(Z.shape[0], 1)\n",
    "        return Z / denoms\n",
    "    \n",
    "    print(\"done\")\n",
    "        \n",
    "    \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([14, 77], dtype=int32)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.array([[1,2, 3],[4,5, 6]]) \n",
    "np.sum(np.power(X, 2), axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is a pre-processing step. Used from the NB multinomial code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "\n",
    "def preprocessing_SST(dictionary_of_phrases_filepath, sentences_filepath, phrases_sentiments_filepath, splits_filepath):\n",
    "    dictionaryDF = pd.read_table(dictionary_of_phrases_filepath, sep = '|', names = (\"phrase\", \"phrase id\"))\n",
    "    sentencesDF = pd.read_table(sentences_filepath, sep = '\\t')\n",
    "    sentimentsDF = pd.read_table(phrases_sentiments_filepath, sep = '|')\n",
    "    splitsDF = pd.read_csv(splitsFP)\n",
    "    \n",
    "    phrases = dict()\n",
    "    for row in range(len(dictionaryDF)):\n",
    "        phrase = dictionaryDF['phrase'][row]\n",
    "        phraseId = dictionaryDF['phrase id'][row]\n",
    "        sentiment = valToLabel(sentimentsDF['sentiment values'][phraseId])\n",
    "        words = phrase.split()\n",
    "        phrases[phrase] = {\n",
    "            \"id\" : phraseId,\n",
    "            \"sentiment\" : sentiment\n",
    "        }\n",
    "    train_docs = list()\n",
    "    dev_docs = list()\n",
    "    test_docs = list()\n",
    "    for id in sentencesDF['sentence_index']:\n",
    "        sentence = sentencesDF['sentence'][id - 1]\n",
    "        if (splitsDF['splitset_label'][id - 1] == 1):\n",
    "            train_docs.append(sentence)\n",
    "        elif (splitsDF['splitset_label'][id - 1] == 2):\n",
    "            test_docs.append(sentence)\n",
    "        else: \n",
    "            dev_docs.append(sentence)  \n",
    "            \n",
    "    training = pairsToPairOfLists(makeInputTuples(train_docs, phrases))\n",
    "    #training[1] = labeled_data_finegrained(training[1])\n",
    "    test = pairsToPairOfLists(makeInputTuples(test_docs, phrases))\n",
    "    #test[1] = labeled_data_finegrained(test[1])\n",
    "    dev = pairsToPairOfLists(makeInputTuples(dev_docs, phrases))\n",
    "    #dev[1] = labeled_data_finegrained(dev[1])\n",
    "    \n",
    "    # MAKE SURE TO CLEAN UP THE LISTS \n",
    "    return training, dev, test\n",
    "\n",
    "def normalize(doc): # given document, returns normalized, negation-tracked version\n",
    "    terminators = {';', '.', '?', '!', '\\n', ':', ','}\n",
    "    negations = {'not', 'no', 'neither', 'never', 'n\\'t'}\n",
    "    sentence = doc.split()\n",
    "    normalized_doc = ''\n",
    "    neg_flag = ''\n",
    "    for word in sentence:\n",
    "        #print('Considering word ', word)\n",
    "        word = neg_flag + word\n",
    "        if word in negations:\n",
    "            neg_flag = '__NOT__'\n",
    "        if word[-1] in terminators:\n",
    "            neg_flag = ''\n",
    "            word = word[0:-1]\n",
    "        normalized_doc = normalized_doc + ' ' + word\n",
    "    return normalized_doc\n",
    "\n",
    "def makeInputTuples(docs, phrases_dictionary): # given documents, returns a tuple (docs, labels) where docs is all documents with a label and labels are corresponding labels\n",
    "    doc_label_pairs = []\n",
    "    for doc in docs:\n",
    "        label = docToLabel(doc, phrases_dictionary)\n",
    "        if label == 'Not found':\n",
    "            continue\n",
    "        else:\n",
    "            doc = normalize(doc)\n",
    "            doc_label_pairs.append((doc, label))\n",
    "    return doc_label_pairs\n",
    "\n",
    "\n",
    "def docToLabel(doc, phrases_dictionary): # given doc, either returns 'Not found' or the appropriate label\n",
    "    if doc not in phrases_dictionary:\n",
    "        return 'Not found'\n",
    "    else:\n",
    "        return phrases_dictionary[doc]['sentiment']\n",
    "    \n",
    "def valToLabel(val):\n",
    "    \n",
    "    if (val <= 0.2):\n",
    "        label = 'very negative'\n",
    "    elif (val <= 0.4):\n",
    "        label = 'negative'\n",
    "    elif (val <= 0.6):\n",
    "        label = 'neutral'\n",
    "    elif (val <= 0.8):\n",
    "        label = 'positive'\n",
    "    else:\n",
    "        label = 'very positive'\n",
    "    return label\n",
    "\n",
    "def pairsToPairOfLists(list_of_pairs):\n",
    "    list1 = []\n",
    "    list2 = []\n",
    "    for pair in list_of_pairs:\n",
    "        list1.append(pair[0])\n",
    "        list2.append(pair[1])\n",
    "    return [list1, list2]\n",
    "\n",
    "def labeled_data_finegrained(labels):  #output Y\n",
    "    conversion = {\n",
    "        'very negative' : 0,\n",
    "        'negative' : 1,\n",
    "        'neutral' : 2,\n",
    "        'positive' : 3,\n",
    "        'very positive' : 4\n",
    "    }\n",
    "    Y = list()\n",
    "    for label in labels:\n",
    "        y = [0, 0, 0, 0, 0]\n",
    "        y[conversion[label]] = 1\n",
    "        Y.append(y)\n",
    "    return Y\n",
    "\n",
    "def maxIndicesToLabels(max_indices):\n",
    "    conversion = {\n",
    "        0 : 'very negative',\n",
    "        1 : 'negative',\n",
    "        2 : 'neutral',\n",
    "        3 : 'positive',\n",
    "        4 : 'very positive'\n",
    "    }\n",
    "    labels = []\n",
    "    for index in max_indices:\n",
    "        labels.append(conversion[index])\n",
    "    return labels\n",
    "        \n",
    "print(\"done\")\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def polarity_error_nb(predictions, labels):\n",
    "   \n",
    "    total = 0\n",
    "    polarity_matches = 0\n",
    "    for i in range(len(labels)):\n",
    "        if labels[i] == 'neutral':\n",
    "            continue\n",
    "        total += 1\n",
    "        if ((labels[i] == 'positive' or labels[i] == 'very positive') \n",
    "            and (predictions[i] == 'positive' or predictions[i] == 'very positive')):\n",
    "            polarity_matches += 1\n",
    "        if ((labels[i] == 'negative' or labels[i] == 'very negative') \n",
    "            and (predictions[i] == 'negative' or predictions[i] == 'very negative')):\n",
    "            polarity_matches += 1\n",
    "    return 1 - polarity_matches / total    \n",
    "    \n",
    "\n",
    "def fine_grained_error(predictions, labels):\n",
    "    matches = 0\n",
    "    for i in range(len(labels)):\n",
    "        if labels[i] == predictions[i]:\n",
    "            matches += 1\n",
    "    return 1 - matches / len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done preprocessing and splitting.\n"
     ]
    }
   ],
   "source": [
    "dictionaryFP = './stanfordSentimentTreebank/dictionary.txt'\n",
    "sentencesFP = './stanfordSentimentTreebank/datasetSentences.txt'\n",
    "sentimentsFP = './stanfordSentimentTreebank/sentiment_labels.txt'\n",
    "splitsFP = './stanfordSentimentTreebank/datasetSplit.txt'\n",
    "\n",
    "train, dev, test = preprocessing_SST(dictionaryFP, sentencesFP, sentimentsFP, splitsFP)\n",
    "print('Done preprocessing and splitting.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building features.\n",
      "Features built.\n",
      "Now converting documents to input matrix.\n",
      "8117 training examples\n",
      "Initiating batch gradient descent with  2000  iterations.\n"
     ]
    }
   ],
   "source": [
    "softmax = SoftmaxRegression(3, 1800, 0.3, 0.03)\n",
    "softmax.train([train[0], labeled_data_finegrained(train[1])])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "answers = softmax.test(test[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labels = maxIndicesToLabels(answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_answers = maxIndicesToLabels(softmax.test(train[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.34019439679817043"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "polarity_error_nb(labels, test[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 3, 1, ..., 0, 3, 3], dtype=int64)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1800, 5)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax.theta.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following plots the cost over number of iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYYAAAD8CAYAAABzTgP2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nUo2CQUREqlEwiIhINQoGERGp5v8DNVNNNili/vAAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x22b39088080>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(softmax.training_costs[0], softmax.training_costs[1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now with the model implemented, we do some hyperparameter coarse-fine tuning. We will do so by generating parameter values for the learning_rate, lambda, number of features, and number of grams to use. For each specified set of parameters, we will train the model and compute the result on the dev set. We will select the parameters that perform the best on the dev set.\n",
    "Note that we begin with the defaults num_grams = 3, num_features = 1800, learning_rate = 0.3, lambda = 0.03"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "defaults = [3, 1800, 0.3, 0.09]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data, train_data_labels, train_sentiment_labels = train[0], labeled_data_finegrained(train[1]), train[1]\n",
    "dev_data, dev_data_labels, dev_sentiment_labels = dev[0], labeled_data_finegrained(dev[1]), dev[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Establish initial results\n",
    "init = SoftmaxRegression(defaults[0], defaults[1], defaults[2], defaults[3])\n",
    "init.train([dev_data, dev_data_labels])\n",
    "labels_init = maxIndicesToLabels(init.test(dev_data))\n",
    "init_results = polarity_error_nb(labels_init, dev_sentiment_labels)\n",
    "\n",
    "# Do a coarse search. \n",
    "for i in range(50):\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
