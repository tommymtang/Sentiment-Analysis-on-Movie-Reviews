{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.util import ngrams\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "# An implementation of Softmax Regression (aka Multinomial logistic regression) classifier. \n",
    "# train and test functions require a pair of lists (document, label), where label is in the format (0,..., 1, ...0) indicating\n",
    "# to which class the label belongs\n",
    "# initiated with n for n-gram \n",
    "\n",
    "# class computes n-grams\n",
    "\n",
    "class SoftmaxRegression:\n",
    "    \n",
    "    def __init__(self, num_grams=3, num_features=1800, learning_rate = 0.3, LAMBDA = 0.03):\n",
    "        self.num_grams = num_grams\n",
    "        self.num_features = num_features  \n",
    "        self.features_built = False\n",
    "        self.learning_rate = learning_rate\n",
    "        self.LAMBDA = LAMBDA\n",
    "        \n",
    "    def train(self, training_set):\n",
    "        print(\"Building features.\")\n",
    "        self.K = len(training_set[1][0])\n",
    "        self.grams_features = self.build_features(training_set[0]) # extends grams_features as necessary\n",
    "        print(\"Features built.\")\n",
    "        print(\"Now converting documents to input matrix.\")\n",
    "        X = self.build_input(training_set[0])\n",
    "        Y = training_set[1]\n",
    "      \n",
    "        self.theta = self.initialize_parameters()\n",
    "        \n",
    "        self.batch_gradient_descent(X, Y, 2000)\n",
    "    \n",
    "    def batch_gradient_descent(self, X, Y, num_iterations = 2000):\n",
    "        m = X.shape[0]\n",
    "        print(m, \"training examples\")\n",
    "        n = self.num_features \n",
    "        self.training_costs = [[],[]]\n",
    "        print(\"Initiating batch gradient descent with \" , num_iterations, \" iterations.\")\n",
    "        for i in range(num_iterations):\n",
    "            H = self.softmax(np.dot(X, self.theta))\n",
    "            if i % 100 == 0:\n",
    "                cost = self.cost(X, Y, H)\n",
    "               # print(\"Loss after \", i, \" iterations is \", cost)\n",
    "                self.training_costs[0].append(i)\n",
    "                self.training_costs[1].append(cost)\n",
    "            grads = self.grads(X, Y, H)\n",
    "            self.theta = self.theta - self.learning_rate * grads\n",
    "            \n",
    "    def test(self, test_set):\n",
    "        test_X = self.build_input(test_set)\n",
    "        test_Y = self.softmax(np.dot(test_X, self.theta)) \n",
    "        max_indices = np.argmax(test_Y, axis = 1)\n",
    "        return max_indices\n",
    "    \n",
    "    def build_input(self, corpus):\n",
    "        X = []\n",
    "        for document in corpus:\n",
    "            X.append(self.doc_to_gram_vec(document))\n",
    "        return np.array(X)\n",
    "    \n",
    "    def build_features(self, corpus):\n",
    "        all_grams = []\n",
    "        for n in range(1, self.num_grams):\n",
    "            ngrams = []\n",
    "            \n",
    "            for doc in corpus:\n",
    "                ngrams.extend(self.compute_grams(doc, n))\n",
    "                \n",
    "            fdist = FreqDist(ngram for ngram in ngrams)\n",
    "            for phrase in fdist.most_common(self.num_features // self.num_grams):\n",
    "                all_grams.append(phrase[0]) # add common n-gram\n",
    "        grams_dict = dict(zip(all_grams, range(len(all_grams)))) # converts into dictionary with positions\n",
    "        self.features_built = True\n",
    "        return grams_dict\n",
    "    \n",
    "    def doc_to_gram_vec(self, doc): # given document, returns vector representing all features\n",
    "        assert self.features_built\n",
    "        doc_vec = np.zeros(self.num_features) \n",
    "        doc_grams = []\n",
    "        for n in range(1, self.num_grams):\n",
    "            doc_grams.extend(self.compute_grams(doc, n))\n",
    "            \n",
    "        for gram in doc_grams:\n",
    "            if gram in self.grams_features:\n",
    "                doc_vec[self.grams_features[gram]] = 1\n",
    "        \n",
    "        return doc_vec\n",
    "        \n",
    "    def compute_grams(self, doc, num_grams):  # given a document, and selected n num_grams, computes all n_grams\n",
    "        tokens = word_tokenize(doc)\n",
    "        if num_grams == 1:\n",
    "            return tokens\n",
    "        else:\n",
    "            return ngrams(tokens, num_grams)        \n",
    "    \n",
    "    def initialize_parameters(self):\n",
    "        theta = np.random.randn(self.num_features, self.K) * 0.01\n",
    "        return theta\n",
    "    \n",
    "    def cost(self, X, Y, H): \n",
    "        m = X.shape[0]\n",
    "        return -(1/m) *(np.sum(np.multiply(Y, H)) + (self.LAMBDA / 2) * np.sum(np.power(self.theta, 2)))\n",
    "    \n",
    "    def grads(self, X, Y, H): # grads will be a matrix\n",
    "        LAMBDA = self.LAMBDA\n",
    "        grads = -np.dot(X.T, (Y-H))\n",
    "        m = X.shape[0]\n",
    "        grads = (1/m) * (grads + LAMBDA * np.absolute(self.theta))\n",
    "        return grads\n",
    "    \n",
    "    def update_parameters(self, grads, learning_rate):\n",
    "        self.theta = self.theta - (learning_rate * grads)\n",
    "        \n",
    "    def sigmoid(self, x):\n",
    "        z = np.exp(x)\n",
    "        z = z / (z+1)\n",
    "        return z\n",
    "    \n",
    "    def predict(self, x):\n",
    "        probs = softmax(np.dot(self.theta.T, x))\n",
    "        return argmax(probs)\n",
    "    \n",
    "    def softmax(self, Z): # Given matrix Z, returns softmax treating each row as a vector\n",
    "        Z = np.exp(Z)\n",
    "        denoms = np.sum(Z, axis = 1)\n",
    "        denoms = denoms.reshape(Z.shape[0], 1)\n",
    "        return Z / denoms\n",
    "    \n",
    "    print(\"done\")\n",
    "        \n",
    "    \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([14, 77], dtype=int32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.array([[1,2, 3],[4,5, 6]]) \n",
    "np.sum(np.power(X, 2), axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is a pre-processing step. Used from the NB multinomial code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "\n",
    "def preprocessing_SST(dictionary_of_phrases_filepath, sentences_filepath, phrases_sentiments_filepath, splits_filepath):\n",
    "    dictionaryDF = pd.read_table(dictionary_of_phrases_filepath, sep = '|', names = (\"phrase\", \"phrase id\"))\n",
    "    sentencesDF = pd.read_table(sentences_filepath, sep = '\\t')\n",
    "    sentimentsDF = pd.read_table(phrases_sentiments_filepath, sep = '|')\n",
    "    splitsDF = pd.read_csv(splitsFP)\n",
    "    \n",
    "    phrases = dict()\n",
    "    for row in range(len(dictionaryDF)):\n",
    "        phrase = dictionaryDF['phrase'][row]\n",
    "        phraseId = dictionaryDF['phrase id'][row]\n",
    "        sentiment = valToLabel(sentimentsDF['sentiment values'][phraseId])\n",
    "        words = phrase.split()\n",
    "        phrases[phrase] = {\n",
    "            \"id\" : phraseId,\n",
    "            \"sentiment\" : sentiment\n",
    "        }\n",
    "    train_docs = list()\n",
    "    dev_docs = list()\n",
    "    test_docs = list()\n",
    "    for id in sentencesDF['sentence_index']:\n",
    "        sentence = sentencesDF['sentence'][id - 1]\n",
    "        if (splitsDF['splitset_label'][id - 1] == 1):\n",
    "            train_docs.append(sentence)\n",
    "        elif (splitsDF['splitset_label'][id - 1] == 2):\n",
    "            test_docs.append(sentence)\n",
    "        else: \n",
    "            dev_docs.append(sentence)  \n",
    "            \n",
    "    training = pairsToPairOfLists(makeInputTuples(train_docs, phrases))\n",
    "    #training[1] = labeled_data_finegrained(training[1])\n",
    "    test = pairsToPairOfLists(makeInputTuples(test_docs, phrases))\n",
    "    #test[1] = labeled_data_finegrained(test[1])\n",
    "    dev = pairsToPairOfLists(makeInputTuples(dev_docs, phrases))\n",
    "    #dev[1] = labeled_data_finegrained(dev[1])\n",
    "    \n",
    "    # MAKE SURE TO CLEAN UP THE LISTS \n",
    "    return training, dev, test\n",
    "\n",
    "def normalize(doc): # given document, returns normalized, negation-tracked version\n",
    "    terminators = {';', '.', '?', '!', '\\n', ':', ','}\n",
    "    negations = {'not', 'no', 'neither', 'never', 'n\\'t'}\n",
    "    sentence = doc.split()\n",
    "    normalized_doc = ''\n",
    "    neg_flag = ''\n",
    "    for word in sentence:\n",
    "        #print('Considering word ', word)\n",
    "        word = neg_flag + word\n",
    "        if word in negations:\n",
    "            neg_flag = '__NOT__'\n",
    "        if word[-1] in terminators:\n",
    "            neg_flag = ''\n",
    "            word = word[0:-1]\n",
    "        normalized_doc = normalized_doc + ' ' + word\n",
    "    return normalized_doc\n",
    "\n",
    "def makeInputTuples(docs, phrases_dictionary): # given documents, returns a tuple (docs, labels) where docs is all documents with a label and labels are corresponding labels\n",
    "    doc_label_pairs = []\n",
    "    for doc in docs:\n",
    "        label = docToLabel(doc, phrases_dictionary)\n",
    "        if label == 'Not found':\n",
    "            continue\n",
    "        else:\n",
    "            doc = normalize(doc)\n",
    "            doc_label_pairs.append((doc, label))\n",
    "    return doc_label_pairs\n",
    "\n",
    "\n",
    "def docToLabel(doc, phrases_dictionary): # given doc, either returns 'Not found' or the appropriate label\n",
    "    if doc not in phrases_dictionary:\n",
    "        return 'Not found'\n",
    "    else:\n",
    "        return phrases_dictionary[doc]['sentiment']\n",
    "    \n",
    "def valToLabel(val):\n",
    "    \n",
    "    if (val <= 0.2):\n",
    "        label = 'very negative'\n",
    "    elif (val <= 0.4):\n",
    "        label = 'negative'\n",
    "    elif (val <= 0.6):\n",
    "        label = 'neutral'\n",
    "    elif (val <= 0.8):\n",
    "        label = 'positive'\n",
    "    else:\n",
    "        label = 'very positive'\n",
    "    return label\n",
    "\n",
    "def pairsToPairOfLists(list_of_pairs):\n",
    "    list1 = []\n",
    "    list2 = []\n",
    "    for pair in list_of_pairs:\n",
    "        list1.append(pair[0])\n",
    "        list2.append(pair[1])\n",
    "    return [list1, list2]\n",
    "\n",
    "def labeled_data_finegrained(labels):  #output Y\n",
    "    conversion = {\n",
    "        'very negative' : 0,\n",
    "        'negative' : 1,\n",
    "        'neutral' : 2,\n",
    "        'positive' : 3,\n",
    "        'very positive' : 4\n",
    "    }\n",
    "    Y = list()\n",
    "    for label in labels:\n",
    "        y = [0, 0, 0, 0, 0]\n",
    "        y[conversion[label]] = 1\n",
    "        Y.append(y)\n",
    "    return Y\n",
    "\n",
    "def maxIndicesToLabels(max_indices):\n",
    "    conversion = {\n",
    "        0 : 'very negative',\n",
    "        1 : 'negative',\n",
    "        2 : 'neutral',\n",
    "        3 : 'positive',\n",
    "        4 : 'very positive'\n",
    "    }\n",
    "    labels = []\n",
    "    for index in max_indices:\n",
    "        labels.append(conversion[index])\n",
    "    return labels\n",
    "        \n",
    "print(\"done\")\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def polarity_error_nb(predictions, labels):\n",
    "   \n",
    "    total = 0\n",
    "    polarity_matches = 0\n",
    "    for i in range(len(labels)):\n",
    "        if labels[i] == 'neutral':\n",
    "            continue\n",
    "        total += 1\n",
    "        if ((labels[i] == 'positive' or labels[i] == 'very positive') \n",
    "            and (predictions[i] == 'positive' or predictions[i] == 'very positive')):\n",
    "            polarity_matches += 1\n",
    "        if ((labels[i] == 'negative' or labels[i] == 'very negative') \n",
    "            and (predictions[i] == 'negative' or predictions[i] == 'very negative')):\n",
    "            polarity_matches += 1\n",
    "    return 1 - polarity_matches / total    \n",
    "    \n",
    "\n",
    "def fine_grained_error(predictions, labels):\n",
    "    matches = 0\n",
    "    for i in range(len(labels)):\n",
    "        if labels[i] == predictions[i]:\n",
    "            matches += 1\n",
    "    return 1 - matches / len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done preprocessing and splitting.\n"
     ]
    }
   ],
   "source": [
    "dictionaryFP = './stanfordSentimentTreebank/dictionary.txt'\n",
    "sentencesFP = './stanfordSentimentTreebank/datasetSentences.txt'\n",
    "sentimentsFP = './stanfordSentimentTreebank/sentiment_labels.txt'\n",
    "splitsFP = './stanfordSentimentTreebank/datasetSplit.txt'\n",
    "\n",
    "train, dev, test = preprocessing_SST(dictionaryFP, sentencesFP, sentimentsFP, splitsFP)\n",
    "print('Done preprocessing and splitting.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building features.\n",
      "Features built.\n",
      "Now converting documents to input matrix.\n",
      "8117 training examples\n",
      "Initiating batch gradient descent with  2000  iterations.\n"
     ]
    }
   ],
   "source": [
    "softmax = SoftmaxRegression(3, 1800, 0.3, 0.03)\n",
    "softmax.train([train[0], labeled_data_finegrained(train[1])])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "answers = softmax.test(test[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labels = maxIndicesToLabels(answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.33847913093196114"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "polarity_error_nb(labels, test[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 3, 3, ..., 0, 3, 4], dtype=int64)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1800, 5)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax.theta.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following plots the cost over number of iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYYAAAD8CAYAAABzTgP2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a0c912d630>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(softmax.training_costs[0], softmax.training_costs[1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now with the model implemented, we do some hyperparameter coarse-fine tuning. We will do so by generating parameter values for the learning_rate, lambda, number of features, and number of grams to use. For each specified set of parameters, we will train the model and compute the result on the dev set. We will select the parameters that perform the best on the dev set.\n",
    "Note that we begin with the defaults num_grams = 3, num_features = 1800, learning_rate = 0.3, lambda = 0.03"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "defaults = [3, 1800, 0.3, 0.09]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data, train_data_labels, train_sentiment_labels = train[0], labeled_data_finegrained(train[1]), train[1]\n",
    "dev_data, dev_data_labels, dev_sentiment_labels = dev[0], labeled_data_finegrained(dev[1]), dev[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Establish initial results\n",
    "init = SoftmaxRegression(defaults[0], defaults[1], defaults[2], defaults[3])\n",
    "init.train([train_data, train_data_labels])\n",
    "labels_init = maxIndicesToLabels(init.test(dev_data))\n",
    "init_results = polarity_error_nb(labels_init, dev_sentiment_labels)\n",
    "\n",
    "best_error = init_results\n",
    "best_params = [defaults]\n",
    "print(\"here\")\n",
    "from random import *\n",
    "\n",
    "# Do a coarse search. \n",
    "for n in range(1,5): # run through possible number of n_grams\n",
    "    for i in range(4):\n",
    "        num_feat = 1200 + i * 300\n",
    "        for j in range(20):\n",
    "            t = -1 * uniform(0,1)\n",
    "            alpha = np.power(10, t)\n",
    "            r = -1 * uniform(2,3)\n",
    "            Lambda = 5 * np.power(10, r)\n",
    "            tuning_run = SoftmaxRegression(n, num_feat, alpha, Lambda)\n",
    "            tuning_run.train([train_data, train_data_labels])\n",
    "            labels = maxIndicesToLabels(tuning_run.test(dev_data))\n",
    "            results = polarity_error_nb(labels, dev_sentiment_labels)\n",
    "            print(\"Classifier with \", n, \"-grams, \", num_feat, \" features, learning rate \", alpha,\n",
    "                  \" and regularization parameter \", Lambda, \" achieved error rate of \", results) \n",
    "            if (results < best_error):\n",
    "                print(\"New best.\")\n",
    "                best_params = [n, num_feat, alpha, Lambda]\n",
    "\n",
    "print(\"Done coarse tuning. Best parameters are \", best_params, \" with a dev error of \", best_error)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "for i in range(1,4):\n",
    "    print (i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
